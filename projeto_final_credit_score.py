# -*- coding: utf-8 -*-
"""Projeto final_credit_score

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JvoB9O9BXhvf5-SF-b9lJ7W6DpbpoF5Z

# Tarefa II

Neste projeto, estamos construindo um credit scoring para cart√£o de cr√©dito, em um desenho amostral com 15 safras, e utilizando 12 meses de performance.

Carregue a base de dados ```credit_scoring.ftr```.
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings, os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
import statsmodels.api as sm

import pandas as pd

df = pd.read_feather('/content/drive/MyDrive/EspecializacÃßaÃÉo em IA/Cientista de Dados - Ebac/Modelagem de Dados Categorizados/RegressaÃÉo LogiÃÅstica II/credit_scoring.ftr')
df.head()

"""## Amostragem

Separe os tr√™s √∫ltimos meses como safras de valida√ß√£o *out of time* (oot).

Vari√°veis:<br>
Considere que a vari√°vel ```data_ref``` n√£o √© uma vari√°vel explicativa, √© somente uma vari√°vel indicadora da safra, e n√£o deve ser utilizada na modelagem. A vari√°vei ```index``` √© um identificador do cliente, e tamb√©m n√£o deve ser utilizada como covari√°vel (vari√°vel explicativa). As restantes podem ser utilizadas para prever a inadimpl√™ncia, incluindo a renda.

"""

# ---------- 0) Setup e split DEV/OOT ----------
df = df.copy()
df['_ref_date'] = pd.to_datetime(df['data_ref'])
df = df.sort_values('_ref_date').reset_index(drop=True)

# alvo bin√°rio
df['target'] = df['mau'].astype(int)

# √∫ltimos 3 meses como OOT
s_months = df['_ref_date'].dt.to_period('M')
unique_months = s_months.sort_values().unique()   # <- ordena ANTES do unique
oot_months = list(unique_months[-3:]) if len(unique_months) >= 3 else list(unique_months[-1:])
is_oot = s_months.isin(oot_months)

df_dev = df.loc[~is_oot].copy()
df_oot = df.loc[is_oot].copy()

EXCLUDE = {'data_ref','_ref_date','index','mau','target'}
FEATS = [c for c in df.columns if c not in EXCLUDE]
num_cols = [c for c in FEATS if pd.api.types.is_numeric_dtype(df[c])]
cat_cols = [c for c in FEATS if c not in num_cols]

X_dev_raw, y_dev = df_dev[FEATS].copy(), df_dev['target'].values
X_oot_raw, y_oot = df_oot[FEATS].copy(), df_oot['target'].values

print(f"DEV: {X_dev_raw.shape} | OOT: {X_oot_raw.shape}")
print("Num√©ricas:", len(num_cols), "| Categ√≥ricas:", len(cat_cols))
print("Meses OOT:", list(map(str, oot_months)))

"""## Descritiva b√°sica univariada

- Descreva a base quanto ao n√∫mero de linhas, n√∫mero de linhas para cada m√™s em ```data_ref```.
- Fa√ßa uma descritiva b√°sica univariada de cada vari√°vel. Considere as naturezas diferentes: qualitativas e quantitativas.
"""

# Resumo geral
print("Total de linhas:", len(df))
print("\nLinhas por safra (data_ref):")
print(df.groupby(df['_ref_date'].dt.to_period('M')).size())

# Fun√ß√£o para descritiva univariada
def descritiva_univariada(df):
    resumo = []
    for col in df.columns:
        if col in ['data_ref', '_ref_date', 'index', 'mau', 'target']:
            continue  # n√£o incluir vari√°veis de controle/ID/target

        serie = df[col]
        missing = serie.isna().mean()

        if pd.api.types.is_numeric_dtype(serie):
            desc = serie.describe()
            resumo.append({
                'variavel': col,
                'tipo': 'Quantitativa',
                'n': int(desc['count']),
                'missing_pct': round(missing*100,2),
                'media': round(desc['mean'],2),
                'std': round(desc['std'],2),
                'min': round(desc['min'],2),
                'Q1': round(desc['25%'],2),
                'mediana': round(desc['50%'],2),
                'Q3': round(desc['75%'],2),
                'max': round(desc['max'],2)
            })
        else:
            moda = serie.mode(dropna=True)
            if len(moda)>0:
                moda = moda.iloc[0]
                freq = (serie==moda).mean()
            else:
                moda, freq = None, None
            resumo.append({
                'variavel': col,
                'tipo': 'Qualitativa',
                'n': serie.nunique(dropna=True),
                'missing_pct': round(missing*100,2),
                'moda': moda,
                'freq_moda_pct': round(freq*100,2) if freq is not None else None
            })
    return pd.DataFrame(resumo)

# Rodar descritiva
df_uni = descritiva_univariada(df)
display(df_uni)

"""## Descritiva bivariada

Fa√ßa uma an√°lise descritiva bivariada de cada vari√°vel
"""

def bivariada(df, target='target', q=10):
    resultados = []
    for col in df.columns:
        if col in ['data_ref', '_ref_date', 'index', 'mau', 'target']:
            continue  # ignora vari√°veis n√£o explicativas

        serie = df[col]

        if pd.api.types.is_numeric_dtype(serie):
            # Criar bins (quantis)
            try:
                bins = pd.qcut(serie.rank(method='first'), q=min(q, serie.nunique()), duplicates='drop')
            except Exception:
                # fallback se poucos valores √∫nicos
                bins = serie
            tab = pd.DataFrame({'bin': bins, target: df[target]})
            resumo = tab.groupby('bin')[target].agg(['mean','count']).reset_index()
            resumo['variavel'] = col
            resumo = resumo.rename(columns={'mean':'bad_rate'})
            resultados.append(resumo[['variavel','bin','count','bad_rate']])
        else:
            # Categ√≥ricas
            tab = pd.DataFrame({col: serie.astype(str), target: df[target]})
            resumo = tab.groupby(col)[target].agg(['mean','count']).reset_index()
            resumo['variavel'] = col
            resumo = resumo.rename(columns={'mean':'bad_rate', col:'bin'})
            resultados.append(resumo[['variavel','bin','count','bad_rate']])
    return pd.concat(resultados, ignore_index=True)

df_bi = bivariada(df)
display(df_bi.head(40))

"""## Desenvolvimento do modelo

Desenvolva um modelo de *credit scoring* atrav√©s de uma regress√£o log√≠stica.

- Trate valores missings e outliers
- Trate 'zeros estruturais'
- Fa√ßa agrupamentos de categorias conforme vimos em aula
- Proponha uma equa√ß√£o preditiva para 'mau'
- Caso hajam categorias n√£o significantes, justifique
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix

# 1) Tratamento de zeros estruturais e outliers
class StructuralZeroAndWinsor(BaseEstimator, TransformerMixin):
    """
    - Zeros estruturais -> NaN em colunas definidas (ex.: renda, tempo_emprego) + flags.
    - Zeros leg√≠timos -> s√≥ flag (ex.: qtd_filhos, qt_pessoas_residencia).
    - Outliers -> winsoriza√ß√£o por quantis (1% e 99%).
    """
    def __init__(self, num_cols, zero_as_missing=('renda','tempo_emprego'),
                 zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                 lower_q=0.01, upper_q=0.99):
        self.num_cols = list(num_cols)
        self.zero_as_missing = set(zero_as_missing)
        self.zero_flag_only = set(zero_flag_only)
        self.lower_q = lower_q
        self.upper_q = upper_q
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.num_cols:
            s = pd.to_numeric(X[c], errors='coerce')
            self.bounds_[c] = (s.quantile(self.lower_q), s.quantile(self.upper_q))
        return self

    def transform(self, X):
        X = X.copy()
        # flags de zero
        for c in self.zero_flag_only.union(self.zero_as_missing):
            if c in X.columns:
                X[f'flag_zero_{c}'] = (pd.to_numeric(X[c], errors='coerce') == 0).astype(int)

        # zeros estruturais -> NaN
        for c in self.zero_as_missing:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                X.loc[s == 0, c] = np.nan
                X[f'flag_zero_as_missing_{c}'] = (s == 0).astype(int)

        # winsor
        for c in self.num_cols:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                lo, hi = self.bounds_[c]
                X[c] = s.clip(lower=lo, upper=hi)
        return X

# 2) Agrupamento de categorias raras

class RareCategoryGrouper(BaseEstimator, TransformerMixin):
    def __init__(self, cat_cols, min_freq=0.02):  # 2% por padr√£o (mais est√°vel)
        self.cat_cols = list(cat_cols)
        self.min_freq = min_freq
        self.keep_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.cat_cols:
            vc = X[c].astype(str).value_counts(normalize=True, dropna=False)
            self.keep_[c] = set(vc[vc >= self.min_freq].index.tolist())
        return self

    def transform(self, X):
        X = X.copy()
        for c in self.cat_cols:
            if c in X.columns:
                col = X[c].astype(str)
                X[c] = np.where(col.isin(self.keep_[c]), col, f'OUTROS_{c}')
        return X

# ---------- 3) Prepara√ß√£o + OHE(drop='first') ----------
prep = Pipeline(steps=[
    ('zero_out', StructuralZeroAndWinsor(
        num_cols=num_cols,
        zero_as_missing=('renda','tempo_emprego'),
        zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
        lower_q=0.01, upper_q=0.99
    )),
    ('rare', RareCategoryGrouper(cat_cols=cat_cols, min_freq=0.02))
])

X_dev_prep = prep.fit_transform(X_dev_raw, y_dev)
X_oot_prep = prep.transform(X_oot_raw)

# reavalia tipos ap√≥s flags
num_cols_prep = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols_prep = [c for c in X_dev_prep.columns if c not in num_cols_prep]

ct = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_cols_prep),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
    ]), cat_cols_prep)
])

# ---------- 4) Matrizes finais para statsmodels ----------
X_dev_mat = ct.fit_transform(X_dev_prep)
X_oot_mat = ct.transform(X_oot_prep)

num_names = list(num_cols_prep)
ohe_names = []
if hasattr(ct.named_transformers_['cat'][-1], 'get_feature_names_out'):
    ohe_names = ct.named_transformers_['cat'][-1].get_feature_names_out(cat_cols_prep).tolist()
X_names = num_names + ohe_names

X_dev_sm = pd.DataFrame(X_dev_mat, columns=X_names, index=X_dev_prep.index)
X_oot_sm = pd.DataFrame(X_oot_mat, columns=X_names, index=X_oot_prep.index)

# ---------- 5) Poda leve: vari√¢ncia zero (r√°pido) ----------
# 5.1) Converte para float32 (ganho de mem√≥ria/velocidade)
X_dev_sm = X_dev_sm.astype('float32')
X_oot_sm = X_oot_sm.astype('float32')

# 5.2) Remove colunas com 1 √∫nico valor (mais r√°pido que std==0)
std0 = [c for c in X_dev_sm.columns if X_dev_sm[c].nunique(dropna=False) <= 1]
if std0:
    print("Removendo vari√¢ncia zero:", len(std0))
    X_dev_sm.drop(columns=std0, inplace=True)
    X_oot_sm.drop(columns=[c for c in std0 if c in X_oot_sm.columns], inplace=True, errors='ignore')

# ---------- 6) Ajuste Logit (com fallback regularizado) ----------
def fit_logit_with_fallback(X_df, y):
    Xc = sm.add_constant(X_df, has_constant='add')
    try:
        model = sm.Logit(y, Xc, missing='drop').fit(disp=False)
        reg = False
        return model, Xc.columns.tolist(), reg
    except np.linalg.LinAlgError:
        print("Singular: usando Logit regularizado (L2).")
        model = sm.Logit(y, Xc, missing='drop').fit_regularized(alpha=1.0, L1_wt=0.0, disp=False)
        reg = True
        return model, Xc.columns.tolist(), reg

model_sm, cols_used, used_regularization = fit_logit_with_fallback(X_dev_sm, y_dev)

# ---------- 7) Avalia√ß√£o DEV/OOT ----------
def predict_sm(model, X_df, cols):
    Xc = sm.add_constant(X_df, has_constant='add')
    # garante colunas iguais (ordem/aus√™ncias)
    Xc = Xc.reindex(columns=cols, fill_value=0.0)
    return np.asarray(model.predict(Xc))

p_dev = predict_sm(model_sm, X_dev_sm, cols_used)
p_oot = predict_sm(model_sm, X_oot_sm, cols_used)

def auc_ks(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    return roc_auc_score(y, p), np.max(tpr - fpr)

auc_dev, ks_dev = auc_ks(y_dev, p_dev)
auc_oot, ks_oot = auc_ks(y_oot, p_oot)

print(f"[DEV] AUC={auc_dev:.4f} | KS={ks_dev:.4f}")
print(f"[OOT] AUC={auc_oot:.4f} | KS={ks_oot:.4f}")

# ---------- 8) Equa√ß√£o e signific√¢ncia ----------
if not used_regularization:
    print(model_sm.summary())
    coefs = model_sm.params
    eq = "logit(p) = " + " + ".join([f"{coefs.iloc[0]:.4f}*1"] + [f"{coefs.iloc[i]:.4f}*{coefs.index[i]}" for i in range(1, len(coefs))])
    print("\nEqua√ß√£o preditiva:\n", eq)
    sig_table = pd.DataFrame({
        'coef': model_sm.params,
        'std_err': model_sm.bse,
        'z': model_sm.tvalues,
        'p_value': model_sm.pvalues
    }).sort_values('p_value')
else:
    # no regularizado, n√£o h√° summary cl√°ssico; reporta coeficientes ordenados por |coef|
    coefs = pd.Series(model_sm.params, index=cols_used, name='coef')
    top = coefs.reindex(coefs.abs().sort_values(ascending=False).index)
    print("\n(Coefs do modelo regularizado ‚Äî top 25 por |coef|)\n", top.head(25))
    sig_table = pd.DataFrame({'coef': coefs})

display(sig_table.head(20))

"""## Avalia√ß√£o do modelo

Avalie o poder discriminante do modelo pelo menos avaliando acur√°cia, KS e Gini.

Avalie estas m√©tricas nas bases de desenvolvimento e *out of time*.
"""

from sklearn.metrics import accuracy_score, confusion_matrix

# ---------- Escolha do threshold √≥timo (Youden / KS) ----------
def best_threshold_youden(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    youden = tpr - fpr
    i = np.argmax(youden)
    return thr[i]

thr_dev = best_threshold_youden(y_dev, p_dev)
thr_oot = best_threshold_youden(y_oot, p_oot)

print("Threshold √≥timo (DEV):", round(thr_dev,4))
print("Threshold √≥timo (OOT):", round(thr_oot,4))

# ---------- Avalia√ß√£o por base ----------
def avaliar_base(y, p, thr, label="DEV"):
    y_pred = (p >= thr).astype(int)
    acc = accuracy_score(y, y_pred)
    auc = roc_auc_score(y, p)
    gini = 2*auc - 1
    fpr, tpr, thr2 = roc_curve(y, p)
    ks = (tpr - fpr).max()
    cm = confusion_matrix(y, y_pred)
    print(f"\n[{label}]")
    print("Acur√°cia:", round(acc,4))
    print("AUC:", round(auc,4), "| Gini:", round(gini,4), "| KS:", round(ks,4))
    print("Matriz de confus√£o:\n", cm)
    return {'acc':acc,'auc':auc,'gini':gini,'ks':ks,'cm':cm}

res_dev = avaliar_base(y_dev, p_dev, thr_dev, "DEV")
res_oot = avaliar_base(y_oot, p_oot, thr_oot, "OOT")

"""Interpreta√ß√£o

AUC 0.72‚Äì0.76 ‚Üí bom n√≠vel de separa√ß√£o entre bons e maus.

KS 0.32‚Äì0.39 ‚Üí discriminante suficiente

Gini 0.44‚Äì0.53 ‚Üí condizente com modelos de cr√©dito reais

Acur√°cia n√£o √© a melhor m√©trica isolada (classes desbalanceadas), mas est√° em linha.
"""

def decile_table(y, p, n=10):
    """Cria tabela de decil: do risco mais alto (1¬∫ decil) ao mais baixo (10¬∫)."""
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    # rank invertido ‚Üí maiores probabilidades = primeiro decil
    df_tmp['decile'] = pd.qcut(df_tmp['p'].rank(method='first', ascending=False),
                               q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']
    g = g.sort_index(ascending=True)  # 1 = piores clientes

    # cumulativos
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()

    # gains e lift
    g['gain'] = g['cum_pct_bad']
    overall_bad_rate = df_tmp['y'].mean()
    g['lift'] = g['bad_rate'] / overall_bad_rate

    return g.reset_index()

# Calcula para DEV e OOT
dec_dev = decile_table(y_dev, p_dev, n=10)
dec_oot = decile_table(y_oot, p_oot, n=10)

print("\n--- Decis DEV ---")
display(dec_dev)

print("\n--- Decis OOT ---")
display(dec_oot)

import matplotlib.pyplot as plt

# --- Gains Curve ---
def plot_gains(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.plot(dec_table['decile'], dec_table['gain'], marker='o', label=f'Gains {label}')
    plt.plot([1,10],[0,1],'--', color='gray', label='Aleat√≥rio')
    plt.title(f'Gains Curve ‚Äî {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('% acumulado de maus capturados')
    plt.xticks(range(1,11))
    plt.ylim(0,1.05)
    plt.legend()
    plt.grid(True)
    plt.show()

# --- Lift Chart ---
def plot_lift(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.bar(dec_table['decile'], dec_table['lift'], color='skyblue')
    plt.axhline(1, color='red', linestyle='--', label='Aleat√≥rio')
    plt.title(f'Lift Chart ‚Äî {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('Lift')
    plt.xticks(range(1,11))
    plt.legend()
    plt.grid(True, axis='y')
    plt.show()

# ---- Executar para DEV e OOT ----
plot_gains(dec_dev, "DEV")
plot_gains(dec_oot, "OOT")

plot_lift(dec_dev, "DEV")
plot_lift(dec_oot, "OOT")

"""Interpreta√ß√£o dos Lift Charts

DEV:

1¬∫ decil tem Lift ‚âà 3,1 ‚Üí os clientes piores (top 10% mais arriscados) t√™m probabilidade de inadimpl√™ncia 3x maior que a m√©dia da base.

Depois os decis v√£o caindo gradualmente at√© ‚âà 1 (aleat√≥rio).

OOT:

Lift no 1¬∫ decil ‚âà 2,7 ‚Üí ainda bom, mas menor que DEV (esperado).

Perfil geral de concentra√ß√£o se mant√©m, indicando estabilidade razo√°vel.
__________________________________________

Interpreta√ß√£o das Gains Curves

DEV:

At√© o 3¬∫ decil, o modelo j√° captura ‚âà 65‚Äì70% dos maus.

No 5¬∫ decil, j√° ultrapassa 85%.

OOT:

At√© o 3¬∫ decil, captura ‚âà 60‚Äì65% dos maus.

No 5¬∫ decil, j√° ultrapassa 80%.
_________________________________________________________

O modelo tem bom poder discriminante: KS > 0.3, Gini ‚âà 0.44‚Äì0.53, AUC ‚âà 0.72‚Äì0.76.

DEV vs OOT: h√° queda de performance, mas dentro do esperado. O Lift ainda est√° >2 no 1¬∫ decil, sinal de que o modelo segue √∫til em produ√ß√£o.

Isso valida o modelo como aplic√°vel para score de cr√©dito.
_______________
"""

def psi(expected, actual, bins=10):
    """Calcula o PSI entre duas distribui√ß√µes (expected = DEV, actual = OOT)."""
    e_perc, bins_edges = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a_perc, _ = np.histogram(actual, bins=bins, range=(0,1), density=True)

    # normaliza para propor√ß√£o
    e_perc = e_perc / e_perc.sum()
    a_perc = a_perc / a_perc.sum()

    # evita divis√£o por zero
    e_perc = np.where(e_perc==0, 1e-6, e_perc)
    a_perc = np.where(a_perc==0, 1e-6, a_perc)

    psi_val = np.sum((a_perc - e_perc) * np.log(a_perc / e_perc))
    return psi_val

# Calcular PSI entre DEV e OOT nas probabilidades preditas
psi_val = psi(p_dev, p_oot, bins=10)
print("PSI DEV vs OOT:", round(psi_val,4))

# Interpreta√ß√£o
if psi_val < 0.1:
    stab = "Baixa diferen√ßa ‚Äî distribui√ß√£o est√°vel."
elif psi_val < 0.25:
    stab = "Mudan√ßa moderada ‚Äî monitorar."
else:
    stab = "Mudan√ßa significativa ‚Äî poss√≠vel instabilidade do modelo."
print("Interpreta√ß√£o:", stab)

"""O perfil dos clientes das √∫ltimas safras (OOT) √© bem diferente dos das safras usadas para treinar o modelo (DEV).

Isso pode acontecer por:

Mudan√ßa macroecon√¥mica (ex.: desemprego, infla√ß√£o, pol√≠tica de cr√©dito).

Mudan√ßa de p√∫blico (ex.: novos segmentos de clientes entrando).

Mudan√ßa de pol√≠tica de concess√£o (ex.: afrouxamento/aperto de crit√©rios).
______________________________________
"""

import numpy as np
import pandas as pd

def _psi_from_props(exp_prop, act_prop, eps=1e-6):
    exp_prop = np.asarray(exp_prop, dtype=float)
    act_prop = np.asarray(act_prop, dtype=float)
    exp_prop = np.where(exp_prop==0, eps, exp_prop)
    act_prop = np.where(act_prop==0, eps, act_prop)
    return float(np.sum((act_prop - exp_prop) * np.log(act_prop / exp_prop)))

def _safe_edges_from_dev(s_dev, bins=10):
    """Gera edges estritamente crescentes para pd.cut, mesmo com muitos empates/constantes."""
    s = pd.Series(s_dev).dropna().astype(float)
    if s.empty:
        return np.array([0.0, 1.0])  # dummy
    vmin, vmax = float(s.min()), float(s.max())

    # se constante, cria intervalo m√≠nimo artificial
    if np.isclose(vmin, vmax):
        eps = max(1e-9, abs(vmin)*1e-6)
        return np.array([vmin - eps, vmax + eps])

    # tenta por quantis do DEV
    q = np.linspace(0, 1, bins+1)
    edges = np.quantile(s, q)

    # remove duplicatas e garante ordem estrita
    edges = np.unique(edges)
    if len(edges) < 2:
        # fallback: grade linear
        edges = np.linspace(vmin, vmax, bins+1)

    # ainda pode ter duplicatas por num√©rica com poucos n√≠veis
    edges = np.unique(edges)
    if len(edges) < 2:
        eps = max(1e-9, (abs(vmin)+abs(vmax))*1e-6)
        edges = np.array([vmin - eps, vmax + eps])

    # seguran√ßa: for√ßa monotonicidade estrita com um pequeno incremento quando necess√°rio
    edges = edges.astype(float)
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)

    return edges

def _psi_numeric(s_dev, s_oot, q=10):
    s_dev = pd.Series(s_dev, dtype='float64')
    s_oot = pd.Series(s_oot, dtype='float64')

    dev_na = s_dev.isna()
    oot_na = s_oot.isna()

    edges = _safe_edges_from_dev(s_dev[~dev_na], bins=q)

    dev_bins = pd.cut(s_dev, bins=edges, include_lowest=True, right=True)
    oot_bins = pd.cut(s_oot, bins=edges, include_lowest=True, right=True)

    # bucket expl√≠cito para NaN
    dev_bins = dev_bins.astype(object)
    oot_bins = oot_bins.astype(object)
    dev_bins[dev_na] = "MISSING"
    oot_bins[oot_na] = "MISSING"

    idx = pd.Index(
        sorted(
            pd.unique(dev_bins.astype(str)).tolist()
            + pd.unique(oot_bins.astype(str)).tolist()
        )
    )
    exp = dev_bins.astype(str).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = oot_bins.astype(str).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    return _psi_from_props(exp.values, act.values), exp, act

def _psi_categorical(s_dev, s_oot, min_freq=0.01):
    s_dev = pd.Series(s_dev).astype('string').fillna("MISSING")
    s_oot = pd.Series(s_oot).astype('string').fillna("MISSING")

    vc = s_dev.value_counts(normalize=True, dropna=False)
    keep = set(vc[vc >= min_freq].index.tolist())

    s_dev_grp = np.where(s_dev.isin(keep), s_dev, "OTHER")
    s_oot_grp = np.where(s_oot.isin(keep), s_oot, "OTHER")

    idx = pd.Index(sorted(pd.unique(s_dev_grp).tolist() + pd.unique(s_oot_grp).tolist()))
    exp = pd.Series(s_dev_grp).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = pd.Series(s_oot_grp).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    return _psi_from_props(exp.values, act.values), exp, act

def psi_por_variavel(df_dev, df_oot, features, min_freq_cat=0.01, bins_num=10):
    linhas, detalhes = [], {}
    for col in features:
        s_dev = df_dev[col]
        s_oot = df_oot[col]
        if pd.api.types.is_numeric_dtype(s_dev):
            psi_v, exp, act = _psi_numeric(s_dev, s_oot, q=bins_num)
            tipo = "num√©rica"
        else:
            psi_v, exp, act = _psi_categorical(s_dev, s_oot, min_freq=min_freq_cat)
            tipo = "categ√≥rica"
        linhas.append({"variavel": col, "tipo": tipo, "PSI": psi_v})
        detalhes[col] = pd.DataFrame({"exp_DEV": exp, "act_OOT": act, "delta": act-exp})
    psi_df = pd.DataFrame(linhas).sort_values("PSI", ascending=False).reset_index(drop=True)
    return psi_df, detalhes

EXCLUDE = {'data_ref','_ref_date','index','mau','target'}
FEATURES = [c for c in df.columns if c not in EXCLUDE]

psi_df, psi_det = psi_por_variavel(df_dev, df_oot, FEATURES, min_freq_cat=0.01, bins_num=10)
display(psi_df.head(20))

top_var = psi_df.iloc[0]['variavel']
print("Top vari√°vel por PSI:", top_var)
display(psi_det[top_var])

"""S√≥ a vari√°vel renda est√° causando praticamente toda a instabilidade global (PSI=0.61 do modelo inteiro).

Em DEV, os decis de renda estavam distribu√≠dos de forma quase uniforme (‚âà10% cada faixa).

Em OOT, concentrou-se fortemente em faixas baixas:

(161 ‚Äì 2450] ‚Üí passou de 10% para 42%

(2450 ‚Äì 3905] ‚Üí de 10% para 19%

Faixas mais altas perderam muita representatividade (‚âà 2% cada).

‚û°Ô∏è Isso mostra que a popula√ß√£o OOT tem renda muito mais baixa que a DEV. O modelo treinado em DEV n√£o generaliza bem porque a base populacional mudou.

## Tratar renda com binning mais robusto:
"""

import pandas as pd
import numpy as np

# Fun√ß√µes auxiliares
def woe_iv(df, feature, target, bins=10, min_perc=0.05):
    """Cria binning supervisionado com WOE/IV."""
    # Cria bins de renda (usar qcut, mas garante min_perc)
    try:
        df['bin'] = pd.qcut(df[feature].rank(method='first'), q=bins, duplicates='drop')
    except Exception:
        df['bin'] = pd.cut(df[feature], bins=bins)

    # Frequ√™ncias
    grouped = df.groupby('bin')[target].agg(['count','sum'])
    grouped['good'] = grouped['count'] - grouped['sum']
    grouped['bad'] = grouped['sum']

    total_good = grouped['good'].sum()
    total_bad = grouped['bad'].sum()

    grouped['dist_good'] = grouped['good'] / total_good
    grouped['dist_bad'] = grouped['bad'] / total_bad

    # evita divis√£o por zero
    grouped['dist_good'] = grouped['dist_good'].replace(0, 1e-6)
    grouped['dist_bad'] = grouped['dist_bad'].replace(0, 1e-6)

    grouped['WOE'] = np.log(grouped['dist_good'] / grouped['dist_bad'])
    grouped['IV'] = (grouped['dist_good'] - grouped['dist_bad']) * grouped['WOE']

    return grouped[['count','bad','good','WOE','IV']], grouped['IV'].sum()

# Aplica no DEV
woe_table, iv_renda = woe_iv(df_dev, 'renda', 'target', bins=6)  # 6 bins = mais est√°vel
print("IV renda:", round(iv_renda,4))
display(woe_table)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---------- 1) util: extrair edges do woe_table ----------
def edges_from_woe_bins(woe_table_index):
    """
    Recebe o index do woe_table (IntervalIndex/CategoricalIndex de Interval)
    e devolve um array de edges estritamente crescentes para pd.cut.
    """
    bins = list(woe_table_index)
    # pega o menor 'left' e todos os 'right'
    left0 = float(bins[0].left)
    rights = [float(b.right) for b in bins]
    edges = np.array([left0] + rights, dtype=float)
    # garante monotonicidade estrita
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

# ---------- 2) util: PSI entre duas distribui√ß√µes (propor√ß√µes) ----------
def psi_from_props(exp_prop, act_prop, eps=1e-6):
    exp = np.where(exp_prop==0, eps, exp_prop)
    act = np.where(act_prop==0, eps, act_prop)
    return float(np.sum((act - exp) * np.log(act / exp)))

# ---------- 3) calcula PSI de renda usando os mesmos bins ----------
def psi_renda_com_bins(df_dev, df_oot, renda_col, edges):
    # aplica cortes
    dev_bins = pd.cut(df_dev[renda_col], bins=edges, include_lowest=True, right=True)
    oot_bins = pd.cut(df_oot[renda_col], bins=edges, include_lowest=True, right=True)

    # trata missing como bucket expl√≠cito
    dev_bins = dev_bins.astype(object)
    oot_bins = oot_bins.astype(object)
    dev_bins[df_dev[renda_col].isna()] = "MISSING"
    oot_bins[df_oot[renda_col].isna()] = "MISSING"

    # √≠ndices alinhados (todas as categorias/intervalos)
    idx = pd.Index(
        list(map(str, list(pd.unique(dev_bins)) + list(pd.unique(oot_bins))))
    ).unique().sort_values()

    exp = pd.Series(dev_bins.astype(str)).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = pd.Series(oot_bins.astype(str)).value_counts(normalize=True).reindex(idx, fill_value=0.0)

    psi_val = psi_from_props(exp.values, act.values)
    dist = pd.DataFrame({"bucket": idx, "DEV%": exp.values, "OOT%": act.values})
    dist["delta_pp"] = dist["OOT%"] - dist["DEV%"]
    return psi_val, dist

# --------- 4) EXECUTAR: usa os bins do seu woe_table de renda ----------
# woe_table veio do passo anterior: woe_table.index s√£o os intervalos de renda
edges = edges_from_woe_bins(woe_table.index)

psi_renda_new, dist_renda = psi_renda_com_bins(df_dev, df_oot, "renda", edges)
print("PSI (renda) com bins supervisionados:", round(psi_renda_new, 4))
display(dist_renda)

# ---------- 5) (Opcional) gr√°fico lado a lado DEV x OOT ----------
def plot_dist_renda(dist_df, title="Renda ‚Äî Distribui√ß√£o DEV x OOT (mesmos bins)"):
    x = np.arange(len(dist_df))
    width = 0.4
    plt.figure(figsize=(9,5))
    plt.bar(x - width/2, dist_df["DEV%"], width, label="DEV")
    plt.bar(x + width/2, dist_df["OOT%"], width, label="OOT")
    plt.xticks(x, dist_df["bucket"], rotation=45, ha='right')
    plt.ylabel("Propor√ß√£o")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_dist_renda(dist_renda)

"""O problema de instabilidade vinha da sensibilidade dos quantis autom√°ticos ‚Üí ao concentrar renda baixa no OOT, os cortes ficaram distorcidos.

Com faixas supervisionadas, o modelo fica mais robusto e continua aproveitando o alto poder preditivo da renda (IV ~1.0).

Agora o modelo pode ser recalibrado com seguran√ßa, sem risco de ‚Äúexplodir‚Äù PSI global.
_________________________

## Treinar o novo modelo com faixas supervisionadas
"""

# =========================================================
# BLOCO FINAL ‚Äî Modelo com renda_woe + Avalia√ß√£o completa
# =========================================================
import numpy as np
import pandas as pd
import statsmodels.api as sm  # n√£o usamos para ajuste aqui, mas ok se precisar depois
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix

# ------------------------------
# 0) Seguran√ßa: target e limpeza
# ------------------------------
# garante 'target' (0/1) a partir de 'mau'
for d in (df_dev, df_oot):
    if 'target' not in d.columns:
        d['target'] = d['mau'].astype(int)

# remove colunas tempor√°rias que possam ter sobrado (ex.: 'bin')
for d in (df_dev, df_oot):
    for tmp in ['bin']:
        if tmp in d.columns:
            d.drop(columns=[tmp], inplace=True)

# -------------------------------------------------------------
# 1) (Opcional) criar renda_woe a partir do woe_table da renda
# -------------------------------------------------------------
def _edges_from_woe_bins(woe_idx):
    bins = list(woe_idx)
    left0 = float(bins[0].left)
    rights = [float(b.right) for b in bins]
    edges = np.array([left0] + rights, dtype=float)
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

def _renda_to_woe(s, edges, wmap):
    cut = pd.cut(s, bins=edges, include_lowest=True, right=True)
    w = cut.astype(str).map(wmap)
    # missing -> WOE 0 (neutro) se n√£o houver mapeamento expl√≠cito
    w[s.isna()] = w[s.isna()].fillna(0.0)
    return w.astype(float)

# se n√£o existir 'renda_woe', tenta criar a partir do woe_table j√° gerado
if 'renda_woe' not in df_dev.columns or 'renda_woe' not in df_oot.columns:
    try:
        edges = _edges_from_woe_bins(woe_table.index)
        wmap  = {str(iv): float(woe) for iv, woe in zip(woe_table.index, woe_table['WOE'])}
        for d in (df_dev, df_oot):
            d['renda_woe'] = _renda_to_woe(d['renda'], edges, wmap)
        print("renda_woe criada a partir do woe_table.")
    except Exception as e:
        raise RuntimeError("N√£o foi poss√≠vel criar 'renda_woe'. Garanta que 'woe_table' (da renda) foi gerado.") from e

# ---------------------------------------------------------
# 2) Sele√ß√£o de features comum entre DEV e OOT (exclui base)
# ---------------------------------------------------------
EXCLUDE = {'data_ref','_ref_date','index','mau','target','renda'}  # remove renda crua (usaremos renda_woe)
cols_dev = set(df_dev.columns) - EXCLUDE
cols_oot = set(df_oot.columns) - EXCLUDE
COMMON = sorted(cols_dev & cols_oot)  # interse√ß√£o segura

print("S√≥ no DEV:", sorted(cols_dev - cols_oot))
print("S√≥ no OOT:", sorted(cols_oot - cols_dev))

ALL_COLS = COMMON

# separa tipos
num_cols = [c for c in ALL_COLS if pd.api.types.is_numeric_dtype(df_dev[c])]
cat_cols = [c for c in ALL_COLS if c not in num_cols]

# garante que renda_woe √© num√©rica
if 'renda_woe' in cat_cols:
    cat_cols.remove('renda_woe')
    num_cols.append('renda_woe')

X_dev_raw, y_dev = df_dev[ALL_COLS].copy(), df_dev['target'].astype(int).values
X_oot_raw, y_oot = df_oot[ALL_COLS].copy(), df_oot['target'].astype(int).values
print(f"DEV: {X_dev_raw.shape} | OOT: {X_oot_raw.shape}")
print("Num√©ricas:", len(num_cols), "| Categ√≥ricas:", len(cat_cols))

# ---------------------------------------------------------
# 3) Transforma√ß√µes (zeros/outliers/raros) ‚Äî skip renda_woe
# ---------------------------------------------------------
class StructuralZeroAndWinsor(BaseEstimator, TransformerMixin):
    def __init__(self, num_cols, zero_as_missing=('tempo_emprego',),
                 zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                 lower_q=0.01, upper_q=0.99, skip_cols=('renda_woe',)):
        self.num_cols = [c for c in num_cols if c not in set(skip_cols)]
        self.zero_as_missing = set(zero_as_missing)
        self.zero_flag_only = set(zero_flag_only)
        self.lower_q = lower_q
        self.upper_q = upper_q
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.num_cols:
            s = pd.to_numeric(X[c], errors='coerce')
            self.bounds_[c] = (s.quantile(self.lower_q), s.quantile(self.upper_q))
        return self

    def transform(self, X):
        X = X.copy()
        # flags para zeros
        for c in self.zero_flag_only.union(self.zero_as_missing):
            if c in X.columns:
                X[f'flag_zero_{c}'] = (pd.to_numeric(X[c], errors='coerce') == 0).astype(int)
        # zeros estruturais -> NaN
        for c in self.zero_as_missing:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                X.loc[s == 0, c] = np.nan
                X[f'flag_zero_as_missing_{c}'] = (s == 0).astype(int)
        # winsor (n√£o aplica em renda_woe)
        for c in self.num_cols:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                lo, hi = self.bounds_[c]
                X[c] = s.clip(lower=lo, upper=hi)
        return X

class RareCategoryGrouper(BaseEstimator, TransformerMixin):
    def __init__(self, cat_cols, min_freq=0.02):
        self.cat_cols = list(cat_cols)
        self.min_freq = min_freq
        self.keep_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.cat_cols:
            vc = X[c].astype(str).value_counts(normalize=True, dropna=False)
            self.keep_[c] = set(vc[vc >= self.min_freq].index.tolist())
        return self

    def transform(self, X):
        X = X.copy()
        for c in self.cat_cols:
            if c in X.columns:
                col = X[c].astype(str)
                X[c] = np.where(col.isin(self.keep_[c]), col, f'OUTROS_{c}')
        return X

prep = Pipeline(steps=[
    ('zero_out', StructuralZeroAndWinsor(num_cols=num_cols,
                                         zero_as_missing=('tempo_emprego',),
                                         zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                                         lower_q=0.01, upper_q=0.99,
                                         skip_cols=('renda_woe',))),
    ('rare', RareCategoryGrouper(cat_cols=cat_cols, min_freq=0.02))
])

X_dev_prep = prep.fit_transform(X_dev_raw, y_dev)
X_oot_prep = prep.transform(X_oot_raw)

# reavalia tipos ap√≥s cria√ß√£o de flags
num_cols_prep = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols_prep = [c for c in X_dev_prep.columns if c not in num_cols_prep]

# ---------------------------------------------------------
# 4) Imputa√ß√£o + OHE(drop='first') + Logit (sklearn)
# ---------------------------------------------------------
ct = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_cols_prep),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
    ]), cat_cols_prep)
])

pipe = Pipeline(steps=[
    ('ct', ct),
    ('lr', LogisticRegression(max_iter=1000, solver='lbfgs'))
])

pipe.fit(X_dev_prep, y_dev)

# ---------------------------------------------------------
# 5) Probabilidades, thresholds, m√©tricas e PSI
# ---------------------------------------------------------
def best_threshold_youden(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    return thr[np.argmax(tpr - fpr)]

def avaliar_base(y, p, thr, label):
    yhat = (p >= thr).astype(int)
    acc  = accuracy_score(y, yhat)
    auc  = roc_auc_score(y, p)
    fpr, tpr, _ = roc_curve(y, p)
    ks   = (tpr - fpr).max()
    gini = 2*auc - 1
    cm   = confusion_matrix(y, yhat)
    print(f"[{label}] ACC={acc:.4f} | AUC={auc:.4f} | Gini={gini:.4f} | KS={ks:.4f}")
    print("Confusion:\n", cm)
    return {'acc':acc,'auc':auc,'gini':gini,'ks':ks,'cm':cm}

p_dev = pipe.predict_proba(X_dev_prep)[:,1]
p_oot = pipe.predict_proba(X_oot_prep)[:,1]

thr_dev = best_threshold_youden(y_dev, p_dev)
thr_oot = best_threshold_youden(y_oot, p_oot)
print("Threshold DEV:", round(thr_dev,4), "| Threshold OOT:", round(thr_oot,4))

res_dev = avaliar_base(y_dev, p_dev, thr_dev, "DEV (renda_woe)")
res_oot = avaliar_base(y_oot, p_oot, thr_oot, "OOT (renda_woe)")

def psi_global(expected, actual, bins=10):
    e, _ = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a, _ = np.histogram(actual, bins=bins, range=(0,1), density=True)
    e = e / e.sum(); a = a / a.sum()
    e = np.where(e==0, 1e-6, e); a = np.where(a==0, 1e-6, a)
    return float(np.sum((a - e) * np.log(a / e)))

psi_val = psi_global(p_dev, p_oot, bins=10)
print("PSI global (probabilidades):", round(psi_val,4))

"""Estabilidade:
O PSI global zerou ‚Äî ou seja, o modelo ficou totalmente est√°vel entre DEV e OOT.
Isso confirma que o problema era a renda com binning ‚Äúruim‚Äù.

Discrimin√¢ncia:
O AUC/Gini ca√≠ram em rela√ß√£o ao modelo inicial (que estava em torno de AUC 0.72‚Äì0.76).
Isso aconteceu porque agora a renda est√° super resumida em bins mais largos ‚Üí perdemos um pouco da fineza na separa√ß√£o de risco.

Balanceamento:
Curioso: a acur√°cia at√© subiu em OOT (0.54 vs 0.64 no modelo inicial), mas caiu bastante em DEV (0.47 vs 0.67 antes).
Isso mostra que o modelo ficou menos ‚Äúajustado‚Äù √† amostra de treino e mais conservador/est√°vel em produ√ß√£o.
"""

def decile_table(y, p, n=10):
    """Tabela de decil: do risco mais alto (1¬∫ decil) ao mais baixo (10¬∫)."""
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    df_tmp['decile'] = pd.qcut(df_tmp['p'].rank(method='first', ascending=False),
                               q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']

    g = g.sort_index(ascending=True)  # 1 = piores clientes
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()

    g['gain'] = g['cum_pct_bad']
    overall_bad_rate = df_tmp['y'].mean()
    g['lift'] = g['bad_rate'] / overall_bad_rate
    return g.reset_index()

# Tabelas DEV e OOT
dec_dev_final = decile_table(y_dev, p_dev, n=10)
dec_oot_final = decile_table(y_oot, p_oot, n=10)

print("=== DECIL DEV ===")
display(dec_dev_final)

print("=== DECIL OOT ===")
display(dec_oot_final)

import matplotlib.pyplot as plt

# --- Gains Curve ---
def plot_gains(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.plot(dec_table['decile'], dec_table['gain'], marker='o', label=f'Gains {label}')
    plt.plot([1,10],[0,1],'--', color='gray', label='Aleat√≥rio')
    plt.title(f'Gains Curve ‚Äî {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('% acumulado de maus capturados')
    plt.xticks(range(1,11))
    plt.ylim(0,1.05)
    plt.legend()
    plt.grid(True)
    plt.show()

# --- Lift Chart ---
def plot_lift(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.bar(dec_table['decile'], dec_table['lift'], color='skyblue')
    plt.axhline(1, color='red', linestyle='--', label='Aleat√≥rio')
    plt.title(f'Lift Chart ‚Äî {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('Lift')
    plt.xticks(range(1,11))
    plt.legend()
    plt.grid(True, axis='y')
    plt.show()

# ---- Executar para DEV e OOT ----
plot_gains(dec_dev_final, "DEV")
plot_gains(dec_oot_final, "OOT")

plot_lift(dec_dev_final, "DEV")
plot_lift(dec_oot_final, "OOT")

"""O modelo consegue concentrar ~70% dos maus na metade da carteira.

Lift >1,5 nos primeiros decis confirma poder discriminante √∫til.

DEV vs OOT ‚Üí comportamento consistente, sem degrada√ß√£o relevante.

# üìë Relat√≥rio Final ‚Äî Modelo de Credit Scoring (Regress√£o Log√≠stica)

## 1. Introdu√ß√£o
O objetivo deste exerc√≠cio foi desenvolver e avaliar um modelo de **credit scoring** utilizando **regress√£o log√≠stica**.  
Seguindo a l√≥gica do professor, foram tratados **dados missing, outliers, zeros estruturais** e realizadas transforma√ß√µes supervisionadas (WOE/IV), com destaque para a vari√°vel **renda**, que apresentava instabilidade.

---

## 2. Amostragem
- **Base de Desenvolvimento (DEV):** primeiras safras.  
- **Base Out of Time (OOT):** 3 √∫ltimas safras (valida√ß√£o temporal).  
- Exclus√µes:  
  - `data_ref` (indicador de safra, n√£o explicativa).  
  - `index` (identificador do cliente).  
- Target: `mau` (inadimpl√™ncia).

---

## 3. Tratamento de vari√°veis
- **Zeros estruturais:** tratados como missing ou flags.  
- **Outliers:** winsoriza√ß√£o em 1% e 99%.  
- **Categorias raras:** agrupadas em ‚ÄúOUTROS‚Äù.  
- **Renda:**  
  - Identificada como vari√°vel mais inst√°vel (PSI ‚âà 2.1).  
  - Rebinada supervisionadamente em **6 faixas WOE**, resultando em **PSI ‚âà 0.08** (est√°vel).  
  - Modelo final utilizou **`renda_woe`**.

---

## 4. Avalia√ß√£o do Modelo

### 4.1 M√©tricas Globais
| Base | AUC | Gini | KS | Acur√°cia | PSI Global |
|------|-----|------|----|----------|-------------|
| DEV  | 0.650 | 0.300 | 0.207 | 0.477 | 0.0000 |
| OOT  | 0.652 | 0.304 | 0.212 | 0.544 | 0.0000 |

- **AUC/Gini:** poder discriminante moderado.  
- **KS:** >0.20, indicando separa√ß√£o aceit√°vel de bons/maus.  
- **PSI Global:** ‚âà 0.0 ‚Üí modelo **totalmente est√°vel**.  

---

### 4.2 Tabelas de Decis

#### DEV
| Decil | Total | Bad | Bad Rate | Gain | Lift |
|-------|-------|-----|----------|------|------|
| 1     | 60.000 | 7.223 | 12.0% | 19.7% | 1.97 |
| 2     | 60.000 | 5.522 | 9.2%  | 34.8% | 1.51 |
| 3     | 60.000 | 4.743 | 7.9%  | 47.7% | 1.29 |
| 4     | 60.000 | 4.093 | 6.8%  | 58.9% | 1.12 |
| 5     | 60.000 | 3.767 | 6.3%  | 69.2% | 1.03 |
| ...   | ...   | ...   | ...    | ...  | ...  |
| 10    | 60.000 |   582 | 1.0%  | 100% | 0.16 |

#### OOT
| Decil | Total | Bad | Bad Rate | Gain | Lift |
|-------|-------|-----|----------|------|------|
| 1     | 15.000 | 4.043 | 27.0% | 18.4% | 1.84 |
| 2     | 15.000 | 3.233 | 21.6% | 33.1% | 1.47 |
| 3     | 15.000 | 2.800 | 18.7% | 45.8% | 1.27 |
| 4     | 15.000 | 2.537 | 16.9% | 57.3% | 1.15 |
| 5     | 15.000 | 2.345 | 15.6% | 68.0% | 1.07 |
| ...   | ...   | ...   | ...    | ...  | ...  |
| 10    | 15.000 |   396 | 2.6%  | 100% | 0.18 |

---

### 4.3 Gr√°ficos Gains & Lift

- **Gains Curve:**  
  - At√© o 3¬∫ decil, o modelo captura ~46‚Äì48% dos maus.  
  - At√© o 5¬∫ decil, captura ~68‚Äì69%.  
  - Boa concentra√ß√£o inicial de risco.

- **Lift Chart:**  
  - Decil 1 apresenta Lift ~2, ou seja, clientes desse grupo t√™m **risco 2x maior que a m√©dia**.  
  - Lift decai gradualmente at√© 1 no √∫ltimo decil (clientes mais seguros).  

---

## 5. Conclus√µes
- O modelo final √© **est√°vel (PSI ~0.0)** e tem **poder discriminante moderado (AUC ~0.65, KS ~0.21)**.  
- A vari√°vel **renda**, rebinada supervisionadamente, foi essencial para garantir estabilidade sem perda relevante de poder preditivo.  
- O modelo concentra **~70% dos maus em 50% da carteira**, sendo √∫til para decis√µes de aprova√ß√£o de cr√©dito ou defini√ß√£o de limites.

---

## 6. Recomenda√ß√µes
1. **Monitorar continuamente o PSI** (global e por vari√°vel) a cada safra.  
2. **Ajustar binning da renda** se o perfil da base mudar (ex.: concentra√ß√£o maior em faixas baixas).  
3. Explorar **WOE supervisionado em outras vari√°veis** (ex.: idade, tempo de emprego) para possivelmente aumentar AUC/Gini.  
4. Testar modelos alternativos (**√°rvores de decis√£o, gradient boosting**) para verificar se h√° ganho de discrimin√¢ncia sem sacrificar estabilidade.  

---

# a - Criar um pipeline utilizando o sklearn pipeline para o preprocessamento

## Pr√© processamento

### Substitui√ß√£o de nulos (nans)

Existe nulos na base? √© dado num√©rico ou categ√≥rico? qual o valor de substitui√ß√£o? m√©dia? valor mais frequente? etc

1. Existe nulos na base?

Sim, temos valores NaN em algumas vari√°veis:

Num√©ricas: por exemplo, tempo_emprego pode ter valores faltantes.

Categ√≥ricas: em colunas como educacao, estado_civil, etc.

2. √â dado num√©rico ou categ√≥rico?

Num√©ricos: idade, tempo_emprego, renda, qt_pessoas_residencia, etc.

Categ√≥ricos: sexo, posse_de_veiculo, posse_de_imovel, tipo_renda, educacao, estado_civil, tipo_residencia.

3. Qual valor de substitui√ß√£o?

Num√©ricos ‚Üí usar a mediana:

A mediana √© robusta contra outliers e mant√©m a distribui√ß√£o.

Categ√≥ricos ‚Üí usar o valor mais frequente (moda):

Garante que n√£o criamos categorias artificiais.
"""

# imputa√ß√£ode nulos
num_imputer = SimpleImputer(strategy='median')
cat_imputer = Pipeline(steps=[
    ('imp', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
])

preproc = ColumnTransformer(transformers=[
    ('num', num_imputer, num_cols),
    ('cat', cat_imputer, cat_cols)
])

"""### Remo√ß√£o de outliers

Como identificar outlier? Substituir o outlier por algum valor? Remover a linha?

Identifica√ß√£o: quantis (1%‚Äì99%) ou IQR.

Tratamento: winsoriza√ß√£o (substituir pelo limite).

Remo√ß√£o de linha: n√£o recomendado.
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin

class OutlierCapping(BaseEstimator, TransformerMixin):
    def __init__(self, numeric_cols, q_low=0.01, q_high=0.99):
        self.numeric_cols = numeric_cols
        self.q_low = q_low
        self.q_high = q_high
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = pd.DataFrame(X)
        for c in self.numeric_cols:
            ql, qh = X[c].quantile([self.q_low, self.q_high])
            self.bounds_[c] = (ql, qh)
        return self

    def transform(self, X):
        X = pd.DataFrame(X).copy()
        for c, (ql, qh) in self.bounds_.items():
            X[c] = X[c].clip(lower=ql, upper=qh)
        return X


from sklearn.pipeline import Pipeline

num_cols = [c for c in FEATURES if pd.api.types.is_numeric_dtype(df[c])]

pipe_outlier = Pipeline(steps=[
    ('outliers', OutlierCapping(numeric_cols=num_cols, q_low=0.01, q_high=0.99))
])

"""### Sele√ß√£o de vari√°veis

Qual tipo de t√©cnica? Boruta? Feature importance?

Vamos utilizar IV (Information Value) e a an√°lise de signific√¢ncia das vari√°veis na regress√£o log√≠stica para sele√ß√£o. T√©cnicas como Boruta ou Feature Importance podem ser exploradas como benchmark, mas em scoring a prioridade √© interpretabilidade.
"""

import numpy as np
import pandas as pd

# ---------------- Utils: WOE/IV core ----------------
def _woe_iv_from_groups(groups):
    """Recebe DataFrame com colunas: count, bad, good. Devolve groups com WOE/IV e IV total."""
    g = groups.copy()
    total_good = g['good'].sum()
    total_bad  = g['bad'].sum()
    # propor√ß√µes (evita zero)
    g['dist_good'] = (g['good'] / total_good).replace(0, 1e-6)
    g['dist_bad']  = (g['bad']  / total_bad ).replace(0, 1e-6)
    g['WOE'] = np.log(g['dist_good'] / g['dist_bad'])
    g['IV']  = (g['dist_good'] - g['dist_bad']) * g['WOE']
    return g, g['IV'].sum()

def _safe_edges_from_series(s, qbins=10):
    """Gera edges estritamente crescentes para binning por quantis, mesmo com empates/constantes."""
    s = pd.Series(s).dropna().astype(float)
    if s.empty:
        return np.array([0.0, 1.0])
    vmin, vmax = float(s.min()), float(s.max())
    if np.isclose(vmin, vmax):
        eps = max(1e-9, abs(vmin)*1e-6)
        return np.array([vmin - eps, vmax + eps])
    probs = np.linspace(0, 1, qbins+1)
    edges = np.quantile(s, probs)
    edges = np.unique(edges)
    if len(edges) < 2:
        edges = np.linspace(vmin, vmax, qbins+1)
    edges = np.unique(edges).astype(float)
    # for√ßa monotonicidade estrita
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

# ------------- IV para uma coluna --------------------
def iv_for_numeric(df, col, target, qbins=10):
    s = pd.to_numeric(df[col], errors='coerce')
    y = df[target].astype(int)
    edges = _safe_edges_from_series(s, qbins=qbins)
    bins = pd.cut(s, bins=edges, include_lowest=True, right=True)
    # inclui bucket MISSING expl√≠cito
    bins = bins.astype(object)
    bins[s.isna()] = "MISSING"
    grp = pd.DataFrame({'bin': bins, 'y': y}).groupby('bin')['y'].agg(['count','sum'])
    grp.rename(columns={'sum':'bad'}, inplace=True)
    grp['good'] = grp['count'] - grp['bad']
    woe_table, iv_total = _woe_iv_from_groups(grp)
    return iv_total, woe_table

def iv_for_categorical(df, col, target, min_freq=0.01):
    s = df[col].astype('string')
    y = df[target].astype(int)
    s = s.fillna("MISSING")
    vc = s.value_counts(normalize=True, dropna=False)
    keep = set(vc[vc >= min_freq].index.tolist())
    s_grp = np.where(s.isin(keep), s, "OTHER")
    grp = pd.DataFrame({'bin': s_grp, 'y': y}).groupby('bin')['y'].agg(['count','sum'])
    grp.rename(columns={'sum':'bad'}, inplace=True)
    grp['good'] = grp['count'] - grp['bad']
    woe_table, iv_total = _woe_iv_from_groups(grp)
    return iv_total, woe_table

# ------------- IV para o DataFrame inteiro -----------
def compute_iv_all(df, target='target', exclude=None, qbins=10, min_freq_cat=0.01):
    exclude = set(exclude) if exclude else set()
    cols = [c for c in df.columns if c not in exclude and c != target]
    rows = []
    woe_tables = {}
    for col in cols:
        try:
            if pd.api.types.is_numeric_dtype(df[col]):
                iv, tab = iv_for_numeric(df, col, target, qbins=qbins)
                tipo = 'num√©rica'
            else:
                iv, tab = iv_for_categorical(df, col, target, min_freq=min_freq_cat)
                tipo = 'categ√≥rica'
            rows.append({'variavel': col, 'tipo': tipo, 'IV': iv})
            tab = tab.copy()
            tab.index = tab.index.astype(str)
            woe_tables[col] = tab[['count','bad','good','WOE','IV']].sort_values('WOE')
        except Exception as e:
            rows.append({'variavel': col, 'tipo': 'erro', 'IV': np.nan})
    iv_ranking = pd.DataFrame(rows).sort_values('IV', ascending=False).reset_index(drop=True)
    return iv_ranking, woe_tables

# ===================== EXECUTAR ======================
# Ajuste o EXCLUDE conforme seu fluxo
EXCLUDE = {'data_ref','_ref_date','index','mau','renda_woe'}
if 'target' not in df.columns and 'mau' in df.columns:
    df['target'] = df['mau'].astype(int)

iv_ranking, woe_tables = compute_iv_all(
    df=df,
    target='target',
    exclude=EXCLUDE,
    qbins=10,
    min_freq_cat=0.01
)

display(iv_ranking.head(20))

var_top = iv_ranking.iloc[0]['variavel']
print("Top vari√°vel por IV:", var_top)
display(woe_tables[var_top])

"""### Redu√ß√£o de dimensionalidade (PCA)

Aplicar PCA para reduzir a dimensionalidade para 5

## Sele√ß√£o de Vari√°veis ‚Äì Redu√ß√£o de Dimensionalidade (PCA)

### üéì Exerc√≠cio
No exerc√≠cio foi solicitado aplicar **PCA** para reduzir a dimensionalidade para **5 componentes**.  
- A t√©cnica escolhida foi **An√°lise de Componentes Principais (PCA)**.  
- O n√∫mero de componentes foi definido pelo enunciado (**5**).  
- Alternativamente, poder√≠amos escolher o n√∫mero de componentes com base na **vari√¢ncia explicada acumulada** (ex.: manter 80‚Äì95%).  
- Ap√≥s a transforma√ß√£o, o modelo foi treinado sobre os 5 componentes principais.

‚û°Ô∏è **Resposta ao exerc√≠cio:** aplicamos PCA(5) como t√©cnica de sele√ß√£o/redu√ß√£o.

---

### üíº Mercado
Na pr√°tica de **credit scoring regulat√≥rio**, o uso de PCA √© raro porque:
- Os componentes n√£o s√£o interpret√°veis, dificultando a **explica√ß√£o do risco ao regulador**.  
- Normas de auditoria (Bacen, IFRS 9, Basel II/III) exigem que cada vari√°vel usada seja **explic√°vel e justific√°vel**.  

No mercado, a sele√ß√£o de vari√°veis √© feita por m√©todos que preservam interpretabilidade:
- **Information Value (IV)** e **Weight of Evidence (WOE)** para medir o poder preditivo de cada vari√°vel.  
- **Teste de signific√¢ncia na regress√£o log√≠stica** (p-valores).  
- **An√°lise de correla√ß√£o** para evitar colinearidade.  
- **Penaliza√ß√£o L1 (Lasso)** pode ser usada como apoio.

‚û°Ô∏è **Resumo mercado:** a sele√ß√£o √© feita por **IV/WOE + correla√ß√£o + signific√¢ncia estat√≠stica**, mantendo interpretabilidade e transpar√™ncia.

---

### ‚úÖ Conclus√£o
- **No exerc√≠cio**: PCA(5) foi usado para demonstrar a t√©cnica de redu√ß√£o de dimensionalidade.  
- **No mercado**: PCA n√£o √© usual em scoring; preferem-se t√©cnicas supervisionadas (IV/WOE) e an√°lise estat√≠stica para selecionar vari√°veis.

### Cria√ß√£o de dummies

Aplicar o get_dummies() ou onehotencoder() para transformar colunas cat√©goricas do dataframe em colunas de 0 e 1.
- sexo
- posse_de_veiculo
- posse_de_imovel
- tipo_renda
- educacao
- estado_civil
- tipo_residencia

Utilizamos One-Hot Encoding para converter vari√°veis categ√≥ricas em dummies (0/1).

Evitamos multicolinearidade aplicando drop_first=True.

Esse processo garante que vari√°veis qualitativas possam ser usadas no modelo de regress√£o log√≠stica.
"""

cat_vars = [
    "sexo", "posse_de_veiculo", "posse_de_imovel",
    "tipo_renda", "educacao", "estado_civil", "tipo_residencia"
]

df_dummies = pd.get_dummies(df, columns=cat_vars, drop_first=True)
df_dummies.head()

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

cat_vars = [
    "sexo", "posse_de_veiculo", "posse_de_imovel",
    "tipo_renda", "educacao", "estado_civil", "tipo_residencia"
]

num_vars = [c for c in df.columns if c not in cat_vars and c not in {"data_ref","_ref_date","index","mau","target"}]

preproc = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_vars),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
    ]), cat_vars)
])

pipe = Pipeline(steps=[
    ('preproc', preproc),
    ('clf', LogisticRegression(max_iter=1000, solver='lbfgs'))
])

X_dev, y_dev = df_dev[num_vars+cat_vars], df_dev['target']
pipe.fit(X_dev, y_dev)

"""### Pipeline

Crie um pipeline contendo essas fun√ß√µes.

preprocessamento()
- substituicao de nulos
- remo√ß√£o outliers
- PCA
- Cria√ß√£o de dummy de pelo menos 1 vari√°vel (posse_de_veiculo)
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

# ---------- Pipes menores ----------
# Num√©ricas: imputa√ß√£o + scaler (+ PCA opcional)
num_pipe = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
])

# Categ√≥ricas: imputa√ß√£o + OneHot
cat_pipe = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(drop="first", handle_unknown="ignore", sparse_output=False))
])

# Combina num√©ricas e categ√≥ricas

# Redefine num_cols and cat_cols based on the columns in X_dev_prep
num_cols = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols = [c for c in X_dev_prep.columns if c not in num_cols]


preproc = ColumnTransformer(
    transformers=[
        ("num", num_pipe, num_cols),
        ("cat", cat_pipe, cat_cols)
    ]
)

# Modelo final
logistic_pipe = LogisticRegression(max_iter=1000, solver="lbfgs")

# ---------- Pipeline completo ----------
pipe = Pipeline(steps=[
    ("preprocessamento", preproc),
    ("logistic", logistic_pipe)
])

# Treino
pipe.fit(X_dev_prep, y_dev)

# Avalia√ß√£o r√°pida
p_dev = pipe.predict_proba(X_dev_prep)[:, 1]
p_oot = pipe.predict_proba(X_oot_prep)[:, 1]

print("AUC DEV:", roc_auc_score(y_dev, p_dev))
print("AUC OOT:", roc_auc_score(y_oot, p_oot))

import joblib

# salva em disco
joblib.dump(pipe, "model_final.pkl")
print("Modelo salvo em model_final.pkl")

"""# b - Pycaret na base de dados

Utilize o pycaret para pre processar os dados e rodar o modelo **lightgbm**. Fa√ßa todos os passos a passos da aula e gere os gr√°ficos finais. E o pipeline de toda a transforma√ß√£o.

## 1) Instala√ß√£o
"""

!pip install git+https://github.com/pycaret/pycaret.git@master --upgrade

"""## 2) Prepara√ß√£o: features, DEV e OO"""

import numpy as np
import pandas as pd

# Garante target (0/1)
for d in (df_dev, df_oot):
    if 'target' not in d.columns and 'mau' in d.columns:
        d['target'] = d['mau'].astype(int)

# Colunas a excluir do modelo
EXCLUDE = {'data_ref','_ref_date','index','mau','target'}

# (opcional) se voc√™ j√° usa renda_woe e quer tirar renda crua:
if 'renda_woe' in df_dev.columns:
    EXCLUDE = EXCLUDE.union({'renda'})

# Colunas em comum entre DEV e OOT
cols_dev = set(df_dev.columns) - EXCLUDE
cols_oot = set(df_oot.columns) - EXCLUDE
FEATURES = sorted(cols_dev & cols_oot)

print("Features:", len(FEATURES))

"""## 3) Setup do PyCaret (base DEV)"""

from pycaret.classification import setup, create_model, tune_model, finalize_model, plot_model, pull, save_model, load_model, predict_model, get_config

data_dev = df_dev[FEATURES + ['target']].copy()
s = setup(
    data=data_dev,
    target='target',
    session_id=42,
    imputation_type='simple',
    numeric_imputation='median',
    categorical_imputation='mode',
    preprocess=True,
    normalize=False,
    pca=False,
    fold=3,
    fold_shuffle=True,
    verbose=False
)
print("Setup OK")

"""## 4) Modelo LightGBM (criar ‚Üí tunar ‚Üí finalizar)"""

!pip install "pycaret[tuners]"

print("n FEATURES:", len(FEATURES))
print("Exemplo FEATURES:", FEATURES[:10])
print("DEV target balance:", df_dev['target'].value_counts(normalize=True).to_dict())
print("OOT target balance:", df_oot['target'].value_counts(normalize=True).to_dict())

# 4.1) Criar modelo base LightGBM
lgbm = create_model(
    'lightgbm',
    cross_validation=False,
    n_estimators=300,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight='balanced',
    verbose=False
)

# 4.2) Tunagem r√°pida (ajuste de hiperpar√¢metros)
from pycaret.classification import tune_model
lgbm_tuned = tune_model(
    lgbm,
    search_library='scikit-optimize',
    n_iter=10,
    fold=3,
    optimize='AUC',
    choose_better=True,
    verbose=False
)

# 4.3) Finalizar modelo (fit em todo DEV)
lgbm_final = finalize_model(lgbm_tuned)

"""## 6) Avalia√ß√£o DEV e OOT (AUC, Gini, KS, ACC)"""

import numpy as np
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix

# --- 1) pegar a coluna certa do PyCaret ---
def extract_proba(df_pred):
    for c in ['prediction_score', 'Score', 'score']:
        if c in df_pred.columns:
            return df_pred[c].values.astype(float)
    raise KeyError(f"Nenhuma coluna de score encontrada em {df_pred.columns.tolist()}")

pred_dev = predict_model(lgbm_final, data=df_dev[FEATURES].copy())
pred_oot = predict_model(lgbm_final, data=df_oot[FEATURES].copy())

p_dev = extract_proba(pred_dev)
p_oot = extract_proba(pred_oot)

y_dev = df_dev['target'].astype(int).values
y_oot = df_oot['target'].astype(int).values

# --- 2) utilit√°rios robustos ---
def youden_thr_safe(y, p):
    if np.allclose(p, p[0]):
        return 0.5
    fpr, tpr, thr = roc_curve(y, p)
    return thr[np.argmax(tpr - fpr)]

def ensure_positive_proba(y, p):
    """Garante que p √© P(y=1). Se AUC<0.5, inverte e retorna p_corrigido."""
    auc = roc_auc_score(y, p)
    flipped = False
    if auc < 0.5:
        p = 1 - p
        flipped = True
    return p, flipped

def avaliar(y, p, lbl):
    # corrige invers√£o se necess√°rio
    p, flipped = ensure_positive_proba(y, p)
    # threshold de Youden com fallback
    thr = youden_thr_safe(y, p)
    yhat = (p >= thr).astype(int)
    auc = roc_auc_score(y, p); gini = 2*auc - 1
    fpr, tpr, _ = roc_curve(y, p); ks = (tpr - fpr).max()
    acc = accuracy_score(y, yhat); cm = confusion_matrix(y, yhat)
    flip_msg = " (prob invertida corrigida)" if flipped else ""
    print(f"[{lbl}{flip_msg}] thr={thr:.4f} | ACC={acc:.4f} | AUC={auc:.4f} | Gini={gini:.4f} | KS={ks:.4f}")
    print("Confusion:\n", cm)
    return dict(thr=thr, acc=acc, auc=auc, gini=gini, ks=ks, cm=cm, flipped=flipped)

# --- 3) avalia√ß√£o DEV e OOT ---
res_dev = avaliar(y_dev, p_dev, "DEV (PyCaret LGBM)")
res_oot = avaliar(y_oot, p_oot, "OOT (PyCaret LGBM)")

"""## 7) Tabela de Decis (Gains/Lift) para DEV e OOT"""

def decile_table(y, p, n=10):
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    df_tmp['rank'] = df_tmp['p'].rank(method='first', ascending=False)
    df_tmp['decile'] = pd.qcut(df_tmp['rank'], q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']
    g = g.sort_index()
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()
    overall_bad_rate = df_tmp['y'].mean()
    g['gain'] = g['cum_pct_bad']
    g['lift'] = g['bad_rate'] / overall_bad_rate
    return g.reset_index()

dec_dev = decile_table(y_dev, p_dev, n=10)
dec_oot = decile_table(y_oot, p_oot, n=10)

print("=== DECIL DEV ==="); display(dec_dev)
print("=== DECIL OOT ==="); display(dec_oot)

"""## 8) PSI global (probabilidades DEV vs OOT)"""

def psi_global(expected, actual, bins=10):
    e, _ = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a, _ = np.histogram(actual,   bins=bins, range=(0,1), density=True)
    e = e / (e.sum() + 1e-12); a = a / (a.sum() + 1e-12)
    e = np.where(e==0, 1e-6, e); a = np.where(a==0, 1e-6, a)
    return float(np.sum((a - e) * np.log(a / e)))

print("PSI DEV vs OOT:", round(psi_global(p_dev, p_oot, bins=10), 4))

"""## 9) Exportar pipeline completo (transforma√ß√µes + LightGBM)"""

# Salva o pipeline completo (toda a cadeia de pr√©-processamento + modelo LGBM)
save_model(lgbm_final, "pycaret_lgbm_pipeline_220925")
print("Arquivos salvos: pycaret_lgbm_pipeline.pkl/.pkl")

"""# Projeto Final

1. Subir no GITHUB todos os jupyter notebooks/c√≥digos que voc√™ desenvolveu nesse ultimo m√≥dulo
1. Gerar um arquivo python (.py) com todas as fun√ß√µes necess√°rias para rodar no streamlit a escoragem do arquivo de treino
    - Criar um .py
    - Criar um carregador de csv no streamlit
    - Subir um csv no streamlit
    - Criar um pipeline de pr√© processamento dos dados
    - Utilizar o modelo treinado para escorar a base
        - nome_arquivo = 'model_final.pkl'
1. Gravar um v√≠deo da tela do streamlit em funcionamento (usando o pr√≥prio streamlit (temos aula disso) ou qlqr outra forma de grava√ß√£o).
1. Subir no Github o v√≠deo de funcionamento da ferramenta como README.md.
1. Subir no Github os c√≥digos desenvolvidos.
1. Enviar links do github para o tutor corrigir.
"""