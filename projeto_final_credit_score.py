# -*- coding: utf-8 -*-
"""Projeto final_credit_score

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JvoB9O9BXhvf5-SF-b9lJ7W6DpbpoF5Z

# Tarefa II

Neste projeto, estamos construindo um credit scoring para cartão de crédito, em um desenho amostral com 15 safras, e utilizando 12 meses de performance.

Carregue a base de dados ```credit_scoring.ftr```.
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings, os
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
import statsmodels.api as sm

import pandas as pd

df = pd.read_feather('/content/drive/MyDrive/Especialização em IA/Cientista de Dados - Ebac/Modelagem de Dados Categorizados/Regressão Logística II/credit_scoring.ftr')
df.head()

"""## Amostragem

Separe os três últimos meses como safras de validação *out of time* (oot).

Variáveis:<br>
Considere que a variável ```data_ref``` não é uma variável explicativa, é somente uma variável indicadora da safra, e não deve ser utilizada na modelagem. A variávei ```index``` é um identificador do cliente, e também não deve ser utilizada como covariável (variável explicativa). As restantes podem ser utilizadas para prever a inadimplência, incluindo a renda.

"""

# ---------- 0) Setup e split DEV/OOT ----------
df = df.copy()
df['_ref_date'] = pd.to_datetime(df['data_ref'])
df = df.sort_values('_ref_date').reset_index(drop=True)

# alvo binário
df['target'] = df['mau'].astype(int)

# últimos 3 meses como OOT
s_months = df['_ref_date'].dt.to_period('M')
unique_months = s_months.sort_values().unique()   # <- ordena ANTES do unique
oot_months = list(unique_months[-3:]) if len(unique_months) >= 3 else list(unique_months[-1:])
is_oot = s_months.isin(oot_months)

df_dev = df.loc[~is_oot].copy()
df_oot = df.loc[is_oot].copy()

EXCLUDE = {'data_ref','_ref_date','index','mau','target'}
FEATS = [c for c in df.columns if c not in EXCLUDE]
num_cols = [c for c in FEATS if pd.api.types.is_numeric_dtype(df[c])]
cat_cols = [c for c in FEATS if c not in num_cols]

X_dev_raw, y_dev = df_dev[FEATS].copy(), df_dev['target'].values
X_oot_raw, y_oot = df_oot[FEATS].copy(), df_oot['target'].values

print(f"DEV: {X_dev_raw.shape} | OOT: {X_oot_raw.shape}")
print("Numéricas:", len(num_cols), "| Categóricas:", len(cat_cols))
print("Meses OOT:", list(map(str, oot_months)))

"""## Descritiva básica univariada

- Descreva a base quanto ao número de linhas, número de linhas para cada mês em ```data_ref```.
- Faça uma descritiva básica univariada de cada variável. Considere as naturezas diferentes: qualitativas e quantitativas.
"""

# Resumo geral
print("Total de linhas:", len(df))
print("\nLinhas por safra (data_ref):")
print(df.groupby(df['_ref_date'].dt.to_period('M')).size())

# Função para descritiva univariada
def descritiva_univariada(df):
    resumo = []
    for col in df.columns:
        if col in ['data_ref', '_ref_date', 'index', 'mau', 'target']:
            continue  # não incluir variáveis de controle/ID/target

        serie = df[col]
        missing = serie.isna().mean()

        if pd.api.types.is_numeric_dtype(serie):
            desc = serie.describe()
            resumo.append({
                'variavel': col,
                'tipo': 'Quantitativa',
                'n': int(desc['count']),
                'missing_pct': round(missing*100,2),
                'media': round(desc['mean'],2),
                'std': round(desc['std'],2),
                'min': round(desc['min'],2),
                'Q1': round(desc['25%'],2),
                'mediana': round(desc['50%'],2),
                'Q3': round(desc['75%'],2),
                'max': round(desc['max'],2)
            })
        else:
            moda = serie.mode(dropna=True)
            if len(moda)>0:
                moda = moda.iloc[0]
                freq = (serie==moda).mean()
            else:
                moda, freq = None, None
            resumo.append({
                'variavel': col,
                'tipo': 'Qualitativa',
                'n': serie.nunique(dropna=True),
                'missing_pct': round(missing*100,2),
                'moda': moda,
                'freq_moda_pct': round(freq*100,2) if freq is not None else None
            })
    return pd.DataFrame(resumo)

# Rodar descritiva
df_uni = descritiva_univariada(df)
display(df_uni)

"""## Descritiva bivariada

Faça uma análise descritiva bivariada de cada variável
"""

def bivariada(df, target='target', q=10):
    resultados = []
    for col in df.columns:
        if col in ['data_ref', '_ref_date', 'index', 'mau', 'target']:
            continue  # ignora variáveis não explicativas

        serie = df[col]

        if pd.api.types.is_numeric_dtype(serie):
            # Criar bins (quantis)
            try:
                bins = pd.qcut(serie.rank(method='first'), q=min(q, serie.nunique()), duplicates='drop')
            except Exception:
                # fallback se poucos valores únicos
                bins = serie
            tab = pd.DataFrame({'bin': bins, target: df[target]})
            resumo = tab.groupby('bin')[target].agg(['mean','count']).reset_index()
            resumo['variavel'] = col
            resumo = resumo.rename(columns={'mean':'bad_rate'})
            resultados.append(resumo[['variavel','bin','count','bad_rate']])
        else:
            # Categóricas
            tab = pd.DataFrame({col: serie.astype(str), target: df[target]})
            resumo = tab.groupby(col)[target].agg(['mean','count']).reset_index()
            resumo['variavel'] = col
            resumo = resumo.rename(columns={'mean':'bad_rate', col:'bin'})
            resultados.append(resumo[['variavel','bin','count','bad_rate']])
    return pd.concat(resultados, ignore_index=True)

df_bi = bivariada(df)
display(df_bi.head(40))

"""## Desenvolvimento do modelo

Desenvolva um modelo de *credit scoring* através de uma regressão logística.

- Trate valores missings e outliers
- Trate 'zeros estruturais'
- Faça agrupamentos de categorias conforme vimos em aula
- Proponha uma equação preditiva para 'mau'
- Caso hajam categorias não significantes, justifique
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix

# 1) Tratamento de zeros estruturais e outliers
class StructuralZeroAndWinsor(BaseEstimator, TransformerMixin):
    """
    - Zeros estruturais -> NaN em colunas definidas (ex.: renda, tempo_emprego) + flags.
    - Zeros legítimos -> só flag (ex.: qtd_filhos, qt_pessoas_residencia).
    - Outliers -> winsorização por quantis (1% e 99%).
    """
    def __init__(self, num_cols, zero_as_missing=('renda','tempo_emprego'),
                 zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                 lower_q=0.01, upper_q=0.99):
        self.num_cols = list(num_cols)
        self.zero_as_missing = set(zero_as_missing)
        self.zero_flag_only = set(zero_flag_only)
        self.lower_q = lower_q
        self.upper_q = upper_q
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.num_cols:
            s = pd.to_numeric(X[c], errors='coerce')
            self.bounds_[c] = (s.quantile(self.lower_q), s.quantile(self.upper_q))
        return self

    def transform(self, X):
        X = X.copy()
        # flags de zero
        for c in self.zero_flag_only.union(self.zero_as_missing):
            if c in X.columns:
                X[f'flag_zero_{c}'] = (pd.to_numeric(X[c], errors='coerce') == 0).astype(int)

        # zeros estruturais -> NaN
        for c in self.zero_as_missing:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                X.loc[s == 0, c] = np.nan
                X[f'flag_zero_as_missing_{c}'] = (s == 0).astype(int)

        # winsor
        for c in self.num_cols:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                lo, hi = self.bounds_[c]
                X[c] = s.clip(lower=lo, upper=hi)
        return X

# 2) Agrupamento de categorias raras

class RareCategoryGrouper(BaseEstimator, TransformerMixin):
    def __init__(self, cat_cols, min_freq=0.02):  # 2% por padrão (mais estável)
        self.cat_cols = list(cat_cols)
        self.min_freq = min_freq
        self.keep_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.cat_cols:
            vc = X[c].astype(str).value_counts(normalize=True, dropna=False)
            self.keep_[c] = set(vc[vc >= self.min_freq].index.tolist())
        return self

    def transform(self, X):
        X = X.copy()
        for c in self.cat_cols:
            if c in X.columns:
                col = X[c].astype(str)
                X[c] = np.where(col.isin(self.keep_[c]), col, f'OUTROS_{c}')
        return X

# ---------- 3) Preparação + OHE(drop='first') ----------
prep = Pipeline(steps=[
    ('zero_out', StructuralZeroAndWinsor(
        num_cols=num_cols,
        zero_as_missing=('renda','tempo_emprego'),
        zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
        lower_q=0.01, upper_q=0.99
    )),
    ('rare', RareCategoryGrouper(cat_cols=cat_cols, min_freq=0.02))
])

X_dev_prep = prep.fit_transform(X_dev_raw, y_dev)
X_oot_prep = prep.transform(X_oot_raw)

# reavalia tipos após flags
num_cols_prep = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols_prep = [c for c in X_dev_prep.columns if c not in num_cols_prep]

ct = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_cols_prep),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
    ]), cat_cols_prep)
])

# ---------- 4) Matrizes finais para statsmodels ----------
X_dev_mat = ct.fit_transform(X_dev_prep)
X_oot_mat = ct.transform(X_oot_prep)

num_names = list(num_cols_prep)
ohe_names = []
if hasattr(ct.named_transformers_['cat'][-1], 'get_feature_names_out'):
    ohe_names = ct.named_transformers_['cat'][-1].get_feature_names_out(cat_cols_prep).tolist()
X_names = num_names + ohe_names

X_dev_sm = pd.DataFrame(X_dev_mat, columns=X_names, index=X_dev_prep.index)
X_oot_sm = pd.DataFrame(X_oot_mat, columns=X_names, index=X_oot_prep.index)

# ---------- 5) Poda leve: variância zero (rápido) ----------
# 5.1) Converte para float32 (ganho de memória/velocidade)
X_dev_sm = X_dev_sm.astype('float32')
X_oot_sm = X_oot_sm.astype('float32')

# 5.2) Remove colunas com 1 único valor (mais rápido que std==0)
std0 = [c for c in X_dev_sm.columns if X_dev_sm[c].nunique(dropna=False) <= 1]
if std0:
    print("Removendo variância zero:", len(std0))
    X_dev_sm.drop(columns=std0, inplace=True)
    X_oot_sm.drop(columns=[c for c in std0 if c in X_oot_sm.columns], inplace=True, errors='ignore')

# ---------- 6) Ajuste Logit (com fallback regularizado) ----------
def fit_logit_with_fallback(X_df, y):
    Xc = sm.add_constant(X_df, has_constant='add')
    try:
        model = sm.Logit(y, Xc, missing='drop').fit(disp=False)
        reg = False
        return model, Xc.columns.tolist(), reg
    except np.linalg.LinAlgError:
        print("Singular: usando Logit regularizado (L2).")
        model = sm.Logit(y, Xc, missing='drop').fit_regularized(alpha=1.0, L1_wt=0.0, disp=False)
        reg = True
        return model, Xc.columns.tolist(), reg

model_sm, cols_used, used_regularization = fit_logit_with_fallback(X_dev_sm, y_dev)

# ---------- 7) Avaliação DEV/OOT ----------
def predict_sm(model, X_df, cols):
    Xc = sm.add_constant(X_df, has_constant='add')
    # garante colunas iguais (ordem/ausências)
    Xc = Xc.reindex(columns=cols, fill_value=0.0)
    return np.asarray(model.predict(Xc))

p_dev = predict_sm(model_sm, X_dev_sm, cols_used)
p_oot = predict_sm(model_sm, X_oot_sm, cols_used)

def auc_ks(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    return roc_auc_score(y, p), np.max(tpr - fpr)

auc_dev, ks_dev = auc_ks(y_dev, p_dev)
auc_oot, ks_oot = auc_ks(y_oot, p_oot)

print(f"[DEV] AUC={auc_dev:.4f} | KS={ks_dev:.4f}")
print(f"[OOT] AUC={auc_oot:.4f} | KS={ks_oot:.4f}")

# ---------- 8) Equação e significância ----------
if not used_regularization:
    print(model_sm.summary())
    coefs = model_sm.params
    eq = "logit(p) = " + " + ".join([f"{coefs.iloc[0]:.4f}*1"] + [f"{coefs.iloc[i]:.4f}*{coefs.index[i]}" for i in range(1, len(coefs))])
    print("\nEquação preditiva:\n", eq)
    sig_table = pd.DataFrame({
        'coef': model_sm.params,
        'std_err': model_sm.bse,
        'z': model_sm.tvalues,
        'p_value': model_sm.pvalues
    }).sort_values('p_value')
else:
    # no regularizado, não há summary clássico; reporta coeficientes ordenados por |coef|
    coefs = pd.Series(model_sm.params, index=cols_used, name='coef')
    top = coefs.reindex(coefs.abs().sort_values(ascending=False).index)
    print("\n(Coefs do modelo regularizado — top 25 por |coef|)\n", top.head(25))
    sig_table = pd.DataFrame({'coef': coefs})

display(sig_table.head(20))

"""## Avaliação do modelo

Avalie o poder discriminante do modelo pelo menos avaliando acurácia, KS e Gini.

Avalie estas métricas nas bases de desenvolvimento e *out of time*.
"""

from sklearn.metrics import accuracy_score, confusion_matrix

# ---------- Escolha do threshold ótimo (Youden / KS) ----------
def best_threshold_youden(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    youden = tpr - fpr
    i = np.argmax(youden)
    return thr[i]

thr_dev = best_threshold_youden(y_dev, p_dev)
thr_oot = best_threshold_youden(y_oot, p_oot)

print("Threshold ótimo (DEV):", round(thr_dev,4))
print("Threshold ótimo (OOT):", round(thr_oot,4))

# ---------- Avaliação por base ----------
def avaliar_base(y, p, thr, label="DEV"):
    y_pred = (p >= thr).astype(int)
    acc = accuracy_score(y, y_pred)
    auc = roc_auc_score(y, p)
    gini = 2*auc - 1
    fpr, tpr, thr2 = roc_curve(y, p)
    ks = (tpr - fpr).max()
    cm = confusion_matrix(y, y_pred)
    print(f"\n[{label}]")
    print("Acurácia:", round(acc,4))
    print("AUC:", round(auc,4), "| Gini:", round(gini,4), "| KS:", round(ks,4))
    print("Matriz de confusão:\n", cm)
    return {'acc':acc,'auc':auc,'gini':gini,'ks':ks,'cm':cm}

res_dev = avaliar_base(y_dev, p_dev, thr_dev, "DEV")
res_oot = avaliar_base(y_oot, p_oot, thr_oot, "OOT")

"""Interpretação

AUC 0.72–0.76 → bom nível de separação entre bons e maus.

KS 0.32–0.39 → discriminante suficiente

Gini 0.44–0.53 → condizente com modelos de crédito reais

Acurácia não é a melhor métrica isolada (classes desbalanceadas), mas está em linha.
"""

def decile_table(y, p, n=10):
    """Cria tabela de decil: do risco mais alto (1º decil) ao mais baixo (10º)."""
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    # rank invertido → maiores probabilidades = primeiro decil
    df_tmp['decile'] = pd.qcut(df_tmp['p'].rank(method='first', ascending=False),
                               q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']
    g = g.sort_index(ascending=True)  # 1 = piores clientes

    # cumulativos
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()

    # gains e lift
    g['gain'] = g['cum_pct_bad']
    overall_bad_rate = df_tmp['y'].mean()
    g['lift'] = g['bad_rate'] / overall_bad_rate

    return g.reset_index()

# Calcula para DEV e OOT
dec_dev = decile_table(y_dev, p_dev, n=10)
dec_oot = decile_table(y_oot, p_oot, n=10)

print("\n--- Decis DEV ---")
display(dec_dev)

print("\n--- Decis OOT ---")
display(dec_oot)

import matplotlib.pyplot as plt

# --- Gains Curve ---
def plot_gains(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.plot(dec_table['decile'], dec_table['gain'], marker='o', label=f'Gains {label}')
    plt.plot([1,10],[0,1],'--', color='gray', label='Aleatório')
    plt.title(f'Gains Curve — {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('% acumulado de maus capturados')
    plt.xticks(range(1,11))
    plt.ylim(0,1.05)
    plt.legend()
    plt.grid(True)
    plt.show()

# --- Lift Chart ---
def plot_lift(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.bar(dec_table['decile'], dec_table['lift'], color='skyblue')
    plt.axhline(1, color='red', linestyle='--', label='Aleatório')
    plt.title(f'Lift Chart — {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('Lift')
    plt.xticks(range(1,11))
    plt.legend()
    plt.grid(True, axis='y')
    plt.show()

# ---- Executar para DEV e OOT ----
plot_gains(dec_dev, "DEV")
plot_gains(dec_oot, "OOT")

plot_lift(dec_dev, "DEV")
plot_lift(dec_oot, "OOT")

"""Interpretação dos Lift Charts

DEV:

1º decil tem Lift ≈ 3,1 → os clientes piores (top 10% mais arriscados) têm probabilidade de inadimplência 3x maior que a média da base.

Depois os decis vão caindo gradualmente até ≈ 1 (aleatório).

OOT:

Lift no 1º decil ≈ 2,7 → ainda bom, mas menor que DEV (esperado).

Perfil geral de concentração se mantém, indicando estabilidade razoável.
__________________________________________

Interpretação das Gains Curves

DEV:

Até o 3º decil, o modelo já captura ≈ 65–70% dos maus.

No 5º decil, já ultrapassa 85%.

OOT:

Até o 3º decil, captura ≈ 60–65% dos maus.

No 5º decil, já ultrapassa 80%.
_________________________________________________________

O modelo tem bom poder discriminante: KS > 0.3, Gini ≈ 0.44–0.53, AUC ≈ 0.72–0.76.

DEV vs OOT: há queda de performance, mas dentro do esperado. O Lift ainda está >2 no 1º decil, sinal de que o modelo segue útil em produção.

Isso valida o modelo como aplicável para score de crédito.
_______________
"""

def psi(expected, actual, bins=10):
    """Calcula o PSI entre duas distribuições (expected = DEV, actual = OOT)."""
    e_perc, bins_edges = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a_perc, _ = np.histogram(actual, bins=bins, range=(0,1), density=True)

    # normaliza para proporção
    e_perc = e_perc / e_perc.sum()
    a_perc = a_perc / a_perc.sum()

    # evita divisão por zero
    e_perc = np.where(e_perc==0, 1e-6, e_perc)
    a_perc = np.where(a_perc==0, 1e-6, a_perc)

    psi_val = np.sum((a_perc - e_perc) * np.log(a_perc / e_perc))
    return psi_val

# Calcular PSI entre DEV e OOT nas probabilidades preditas
psi_val = psi(p_dev, p_oot, bins=10)
print("PSI DEV vs OOT:", round(psi_val,4))

# Interpretação
if psi_val < 0.1:
    stab = "Baixa diferença — distribuição estável."
elif psi_val < 0.25:
    stab = "Mudança moderada — monitorar."
else:
    stab = "Mudança significativa — possível instabilidade do modelo."
print("Interpretação:", stab)

"""O perfil dos clientes das últimas safras (OOT) é bem diferente dos das safras usadas para treinar o modelo (DEV).

Isso pode acontecer por:

Mudança macroeconômica (ex.: desemprego, inflação, política de crédito).

Mudança de público (ex.: novos segmentos de clientes entrando).

Mudança de política de concessão (ex.: afrouxamento/aperto de critérios).
______________________________________
"""

import numpy as np
import pandas as pd

def _psi_from_props(exp_prop, act_prop, eps=1e-6):
    exp_prop = np.asarray(exp_prop, dtype=float)
    act_prop = np.asarray(act_prop, dtype=float)
    exp_prop = np.where(exp_prop==0, eps, exp_prop)
    act_prop = np.where(act_prop==0, eps, act_prop)
    return float(np.sum((act_prop - exp_prop) * np.log(act_prop / exp_prop)))

def _safe_edges_from_dev(s_dev, bins=10):
    """Gera edges estritamente crescentes para pd.cut, mesmo com muitos empates/constantes."""
    s = pd.Series(s_dev).dropna().astype(float)
    if s.empty:
        return np.array([0.0, 1.0])  # dummy
    vmin, vmax = float(s.min()), float(s.max())

    # se constante, cria intervalo mínimo artificial
    if np.isclose(vmin, vmax):
        eps = max(1e-9, abs(vmin)*1e-6)
        return np.array([vmin - eps, vmax + eps])

    # tenta por quantis do DEV
    q = np.linspace(0, 1, bins+1)
    edges = np.quantile(s, q)

    # remove duplicatas e garante ordem estrita
    edges = np.unique(edges)
    if len(edges) < 2:
        # fallback: grade linear
        edges = np.linspace(vmin, vmax, bins+1)

    # ainda pode ter duplicatas por numérica com poucos níveis
    edges = np.unique(edges)
    if len(edges) < 2:
        eps = max(1e-9, (abs(vmin)+abs(vmax))*1e-6)
        edges = np.array([vmin - eps, vmax + eps])

    # segurança: força monotonicidade estrita com um pequeno incremento quando necessário
    edges = edges.astype(float)
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)

    return edges

def _psi_numeric(s_dev, s_oot, q=10):
    s_dev = pd.Series(s_dev, dtype='float64')
    s_oot = pd.Series(s_oot, dtype='float64')

    dev_na = s_dev.isna()
    oot_na = s_oot.isna()

    edges = _safe_edges_from_dev(s_dev[~dev_na], bins=q)

    dev_bins = pd.cut(s_dev, bins=edges, include_lowest=True, right=True)
    oot_bins = pd.cut(s_oot, bins=edges, include_lowest=True, right=True)

    # bucket explícito para NaN
    dev_bins = dev_bins.astype(object)
    oot_bins = oot_bins.astype(object)
    dev_bins[dev_na] = "MISSING"
    oot_bins[oot_na] = "MISSING"

    idx = pd.Index(
        sorted(
            pd.unique(dev_bins.astype(str)).tolist()
            + pd.unique(oot_bins.astype(str)).tolist()
        )
    )
    exp = dev_bins.astype(str).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = oot_bins.astype(str).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    return _psi_from_props(exp.values, act.values), exp, act

def _psi_categorical(s_dev, s_oot, min_freq=0.01):
    s_dev = pd.Series(s_dev).astype('string').fillna("MISSING")
    s_oot = pd.Series(s_oot).astype('string').fillna("MISSING")

    vc = s_dev.value_counts(normalize=True, dropna=False)
    keep = set(vc[vc >= min_freq].index.tolist())

    s_dev_grp = np.where(s_dev.isin(keep), s_dev, "OTHER")
    s_oot_grp = np.where(s_oot.isin(keep), s_oot, "OTHER")

    idx = pd.Index(sorted(pd.unique(s_dev_grp).tolist() + pd.unique(s_oot_grp).tolist()))
    exp = pd.Series(s_dev_grp).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = pd.Series(s_oot_grp).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    return _psi_from_props(exp.values, act.values), exp, act

def psi_por_variavel(df_dev, df_oot, features, min_freq_cat=0.01, bins_num=10):
    linhas, detalhes = [], {}
    for col in features:
        s_dev = df_dev[col]
        s_oot = df_oot[col]
        if pd.api.types.is_numeric_dtype(s_dev):
            psi_v, exp, act = _psi_numeric(s_dev, s_oot, q=bins_num)
            tipo = "numérica"
        else:
            psi_v, exp, act = _psi_categorical(s_dev, s_oot, min_freq=min_freq_cat)
            tipo = "categórica"
        linhas.append({"variavel": col, "tipo": tipo, "PSI": psi_v})
        detalhes[col] = pd.DataFrame({"exp_DEV": exp, "act_OOT": act, "delta": act-exp})
    psi_df = pd.DataFrame(linhas).sort_values("PSI", ascending=False).reset_index(drop=True)
    return psi_df, detalhes

EXCLUDE = {'data_ref','_ref_date','index','mau','target'}
FEATURES = [c for c in df.columns if c not in EXCLUDE]

psi_df, psi_det = psi_por_variavel(df_dev, df_oot, FEATURES, min_freq_cat=0.01, bins_num=10)
display(psi_df.head(20))

top_var = psi_df.iloc[0]['variavel']
print("Top variável por PSI:", top_var)
display(psi_det[top_var])

"""Só a variável renda está causando praticamente toda a instabilidade global (PSI=0.61 do modelo inteiro).

Em DEV, os decis de renda estavam distribuídos de forma quase uniforme (≈10% cada faixa).

Em OOT, concentrou-se fortemente em faixas baixas:

(161 – 2450] → passou de 10% para 42%

(2450 – 3905] → de 10% para 19%

Faixas mais altas perderam muita representatividade (≈ 2% cada).

➡️ Isso mostra que a população OOT tem renda muito mais baixa que a DEV. O modelo treinado em DEV não generaliza bem porque a base populacional mudou.

## Tratar renda com binning mais robusto:
"""

import pandas as pd
import numpy as np

# Funções auxiliares
def woe_iv(df, feature, target, bins=10, min_perc=0.05):
    """Cria binning supervisionado com WOE/IV."""
    # Cria bins de renda (usar qcut, mas garante min_perc)
    try:
        df['bin'] = pd.qcut(df[feature].rank(method='first'), q=bins, duplicates='drop')
    except Exception:
        df['bin'] = pd.cut(df[feature], bins=bins)

    # Frequências
    grouped = df.groupby('bin')[target].agg(['count','sum'])
    grouped['good'] = grouped['count'] - grouped['sum']
    grouped['bad'] = grouped['sum']

    total_good = grouped['good'].sum()
    total_bad = grouped['bad'].sum()

    grouped['dist_good'] = grouped['good'] / total_good
    grouped['dist_bad'] = grouped['bad'] / total_bad

    # evita divisão por zero
    grouped['dist_good'] = grouped['dist_good'].replace(0, 1e-6)
    grouped['dist_bad'] = grouped['dist_bad'].replace(0, 1e-6)

    grouped['WOE'] = np.log(grouped['dist_good'] / grouped['dist_bad'])
    grouped['IV'] = (grouped['dist_good'] - grouped['dist_bad']) * grouped['WOE']

    return grouped[['count','bad','good','WOE','IV']], grouped['IV'].sum()

# Aplica no DEV
woe_table, iv_renda = woe_iv(df_dev, 'renda', 'target', bins=6)  # 6 bins = mais estável
print("IV renda:", round(iv_renda,4))
display(woe_table)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---------- 1) util: extrair edges do woe_table ----------
def edges_from_woe_bins(woe_table_index):
    """
    Recebe o index do woe_table (IntervalIndex/CategoricalIndex de Interval)
    e devolve um array de edges estritamente crescentes para pd.cut.
    """
    bins = list(woe_table_index)
    # pega o menor 'left' e todos os 'right'
    left0 = float(bins[0].left)
    rights = [float(b.right) for b in bins]
    edges = np.array([left0] + rights, dtype=float)
    # garante monotonicidade estrita
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

# ---------- 2) util: PSI entre duas distribuições (proporções) ----------
def psi_from_props(exp_prop, act_prop, eps=1e-6):
    exp = np.where(exp_prop==0, eps, exp_prop)
    act = np.where(act_prop==0, eps, act_prop)
    return float(np.sum((act - exp) * np.log(act / exp)))

# ---------- 3) calcula PSI de renda usando os mesmos bins ----------
def psi_renda_com_bins(df_dev, df_oot, renda_col, edges):
    # aplica cortes
    dev_bins = pd.cut(df_dev[renda_col], bins=edges, include_lowest=True, right=True)
    oot_bins = pd.cut(df_oot[renda_col], bins=edges, include_lowest=True, right=True)

    # trata missing como bucket explícito
    dev_bins = dev_bins.astype(object)
    oot_bins = oot_bins.astype(object)
    dev_bins[df_dev[renda_col].isna()] = "MISSING"
    oot_bins[df_oot[renda_col].isna()] = "MISSING"

    # índices alinhados (todas as categorias/intervalos)
    idx = pd.Index(
        list(map(str, list(pd.unique(dev_bins)) + list(pd.unique(oot_bins))))
    ).unique().sort_values()

    exp = pd.Series(dev_bins.astype(str)).value_counts(normalize=True).reindex(idx, fill_value=0.0)
    act = pd.Series(oot_bins.astype(str)).value_counts(normalize=True).reindex(idx, fill_value=0.0)

    psi_val = psi_from_props(exp.values, act.values)
    dist = pd.DataFrame({"bucket": idx, "DEV%": exp.values, "OOT%": act.values})
    dist["delta_pp"] = dist["OOT%"] - dist["DEV%"]
    return psi_val, dist

# --------- 4) EXECUTAR: usa os bins do seu woe_table de renda ----------
# woe_table veio do passo anterior: woe_table.index são os intervalos de renda
edges = edges_from_woe_bins(woe_table.index)

psi_renda_new, dist_renda = psi_renda_com_bins(df_dev, df_oot, "renda", edges)
print("PSI (renda) com bins supervisionados:", round(psi_renda_new, 4))
display(dist_renda)

# ---------- 5) (Opcional) gráfico lado a lado DEV x OOT ----------
def plot_dist_renda(dist_df, title="Renda — Distribuição DEV x OOT (mesmos bins)"):
    x = np.arange(len(dist_df))
    width = 0.4
    plt.figure(figsize=(9,5))
    plt.bar(x - width/2, dist_df["DEV%"], width, label="DEV")
    plt.bar(x + width/2, dist_df["OOT%"], width, label="OOT")
    plt.xticks(x, dist_df["bucket"], rotation=45, ha='right')
    plt.ylabel("Proporção")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_dist_renda(dist_renda)

"""O problema de instabilidade vinha da sensibilidade dos quantis automáticos → ao concentrar renda baixa no OOT, os cortes ficaram distorcidos.

Com faixas supervisionadas, o modelo fica mais robusto e continua aproveitando o alto poder preditivo da renda (IV ~1.0).

Agora o modelo pode ser recalibrado com segurança, sem risco de “explodir” PSI global.
_________________________

## Treinar o novo modelo com faixas supervisionadas
"""

# =========================================================
# BLOCO FINAL — Modelo com renda_woe + Avaliação completa
# =========================================================
import numpy as np
import pandas as pd
import statsmodels.api as sm  # não usamos para ajuste aqui, mas ok se precisar depois
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix

# ------------------------------
# 0) Segurança: target e limpeza
# ------------------------------
# garante 'target' (0/1) a partir de 'mau'
for d in (df_dev, df_oot):
    if 'target' not in d.columns:
        d['target'] = d['mau'].astype(int)

# remove colunas temporárias que possam ter sobrado (ex.: 'bin')
for d in (df_dev, df_oot):
    for tmp in ['bin']:
        if tmp in d.columns:
            d.drop(columns=[tmp], inplace=True)

# -------------------------------------------------------------
# 1) (Opcional) criar renda_woe a partir do woe_table da renda
# -------------------------------------------------------------
def _edges_from_woe_bins(woe_idx):
    bins = list(woe_idx)
    left0 = float(bins[0].left)
    rights = [float(b.right) for b in bins]
    edges = np.array([left0] + rights, dtype=float)
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

def _renda_to_woe(s, edges, wmap):
    cut = pd.cut(s, bins=edges, include_lowest=True, right=True)
    w = cut.astype(str).map(wmap)
    # missing -> WOE 0 (neutro) se não houver mapeamento explícito
    w[s.isna()] = w[s.isna()].fillna(0.0)
    return w.astype(float)

# se não existir 'renda_woe', tenta criar a partir do woe_table já gerado
if 'renda_woe' not in df_dev.columns or 'renda_woe' not in df_oot.columns:
    try:
        edges = _edges_from_woe_bins(woe_table.index)
        wmap  = {str(iv): float(woe) for iv, woe in zip(woe_table.index, woe_table['WOE'])}
        for d in (df_dev, df_oot):
            d['renda_woe'] = _renda_to_woe(d['renda'], edges, wmap)
        print("renda_woe criada a partir do woe_table.")
    except Exception as e:
        raise RuntimeError("Não foi possível criar 'renda_woe'. Garanta que 'woe_table' (da renda) foi gerado.") from e

# ---------------------------------------------------------
# 2) Seleção de features comum entre DEV e OOT (exclui base)
# ---------------------------------------------------------
EXCLUDE = {'data_ref','_ref_date','index','mau','target','renda'}  # remove renda crua (usaremos renda_woe)
cols_dev = set(df_dev.columns) - EXCLUDE
cols_oot = set(df_oot.columns) - EXCLUDE
COMMON = sorted(cols_dev & cols_oot)  # interseção segura

print("Só no DEV:", sorted(cols_dev - cols_oot))
print("Só no OOT:", sorted(cols_oot - cols_dev))

ALL_COLS = COMMON

# separa tipos
num_cols = [c for c in ALL_COLS if pd.api.types.is_numeric_dtype(df_dev[c])]
cat_cols = [c for c in ALL_COLS if c not in num_cols]

# garante que renda_woe é numérica
if 'renda_woe' in cat_cols:
    cat_cols.remove('renda_woe')
    num_cols.append('renda_woe')

X_dev_raw, y_dev = df_dev[ALL_COLS].copy(), df_dev['target'].astype(int).values
X_oot_raw, y_oot = df_oot[ALL_COLS].copy(), df_oot['target'].astype(int).values
print(f"DEV: {X_dev_raw.shape} | OOT: {X_oot_raw.shape}")
print("Numéricas:", len(num_cols), "| Categóricas:", len(cat_cols))

# ---------------------------------------------------------
# 3) Transformações (zeros/outliers/raros) — skip renda_woe
# ---------------------------------------------------------
class StructuralZeroAndWinsor(BaseEstimator, TransformerMixin):
    def __init__(self, num_cols, zero_as_missing=('tempo_emprego',),
                 zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                 lower_q=0.01, upper_q=0.99, skip_cols=('renda_woe',)):
        self.num_cols = [c for c in num_cols if c not in set(skip_cols)]
        self.zero_as_missing = set(zero_as_missing)
        self.zero_flag_only = set(zero_flag_only)
        self.lower_q = lower_q
        self.upper_q = upper_q
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.num_cols:
            s = pd.to_numeric(X[c], errors='coerce')
            self.bounds_[c] = (s.quantile(self.lower_q), s.quantile(self.upper_q))
        return self

    def transform(self, X):
        X = X.copy()
        # flags para zeros
        for c in self.zero_flag_only.union(self.zero_as_missing):
            if c in X.columns:
                X[f'flag_zero_{c}'] = (pd.to_numeric(X[c], errors='coerce') == 0).astype(int)
        # zeros estruturais -> NaN
        for c in self.zero_as_missing:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                X.loc[s == 0, c] = np.nan
                X[f'flag_zero_as_missing_{c}'] = (s == 0).astype(int)
        # winsor (não aplica em renda_woe)
        for c in self.num_cols:
            if c in X.columns:
                s = pd.to_numeric(X[c], errors='coerce')
                lo, hi = self.bounds_[c]
                X[c] = s.clip(lower=lo, upper=hi)
        return X

class RareCategoryGrouper(BaseEstimator, TransformerMixin):
    def __init__(self, cat_cols, min_freq=0.02):
        self.cat_cols = list(cat_cols)
        self.min_freq = min_freq
        self.keep_ = {}

    def fit(self, X, y=None):
        X = X.copy()
        for c in self.cat_cols:
            vc = X[c].astype(str).value_counts(normalize=True, dropna=False)
            self.keep_[c] = set(vc[vc >= self.min_freq].index.tolist())
        return self

    def transform(self, X):
        X = X.copy()
        for c in self.cat_cols:
            if c in X.columns:
                col = X[c].astype(str)
                X[c] = np.where(col.isin(self.keep_[c]), col, f'OUTROS_{c}')
        return X

prep = Pipeline(steps=[
    ('zero_out', StructuralZeroAndWinsor(num_cols=num_cols,
                                         zero_as_missing=('tempo_emprego',),
                                         zero_flag_only=('qtd_filhos','qt_pessoas_residencia'),
                                         lower_q=0.01, upper_q=0.99,
                                         skip_cols=('renda_woe',))),
    ('rare', RareCategoryGrouper(cat_cols=cat_cols, min_freq=0.02))
])

X_dev_prep = prep.fit_transform(X_dev_raw, y_dev)
X_oot_prep = prep.transform(X_oot_raw)

# reavalia tipos após criação de flags
num_cols_prep = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols_prep = [c for c in X_dev_prep.columns if c not in num_cols_prep]

# ---------------------------------------------------------
# 4) Imputação + OHE(drop='first') + Logit (sklearn)
# ---------------------------------------------------------
ct = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_cols_prep),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
    ]), cat_cols_prep)
])

pipe = Pipeline(steps=[
    ('ct', ct),
    ('lr', LogisticRegression(max_iter=1000, solver='lbfgs'))
])

pipe.fit(X_dev_prep, y_dev)

# ---------------------------------------------------------
# 5) Probabilidades, thresholds, métricas e PSI
# ---------------------------------------------------------
def best_threshold_youden(y, p):
    fpr, tpr, thr = roc_curve(y, p)
    return thr[np.argmax(tpr - fpr)]

def avaliar_base(y, p, thr, label):
    yhat = (p >= thr).astype(int)
    acc  = accuracy_score(y, yhat)
    auc  = roc_auc_score(y, p)
    fpr, tpr, _ = roc_curve(y, p)
    ks   = (tpr - fpr).max()
    gini = 2*auc - 1
    cm   = confusion_matrix(y, yhat)
    print(f"[{label}] ACC={acc:.4f} | AUC={auc:.4f} | Gini={gini:.4f} | KS={ks:.4f}")
    print("Confusion:\n", cm)
    return {'acc':acc,'auc':auc,'gini':gini,'ks':ks,'cm':cm}

p_dev = pipe.predict_proba(X_dev_prep)[:,1]
p_oot = pipe.predict_proba(X_oot_prep)[:,1]

thr_dev = best_threshold_youden(y_dev, p_dev)
thr_oot = best_threshold_youden(y_oot, p_oot)
print("Threshold DEV:", round(thr_dev,4), "| Threshold OOT:", round(thr_oot,4))

res_dev = avaliar_base(y_dev, p_dev, thr_dev, "DEV (renda_woe)")
res_oot = avaliar_base(y_oot, p_oot, thr_oot, "OOT (renda_woe)")

def psi_global(expected, actual, bins=10):
    e, _ = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a, _ = np.histogram(actual, bins=bins, range=(0,1), density=True)
    e = e / e.sum(); a = a / a.sum()
    e = np.where(e==0, 1e-6, e); a = np.where(a==0, 1e-6, a)
    return float(np.sum((a - e) * np.log(a / e)))

psi_val = psi_global(p_dev, p_oot, bins=10)
print("PSI global (probabilidades):", round(psi_val,4))

"""Estabilidade:
O PSI global zerou — ou seja, o modelo ficou totalmente estável entre DEV e OOT.
Isso confirma que o problema era a renda com binning “ruim”.

Discriminância:
O AUC/Gini caíram em relação ao modelo inicial (que estava em torno de AUC 0.72–0.76).
Isso aconteceu porque agora a renda está super resumida em bins mais largos → perdemos um pouco da fineza na separação de risco.

Balanceamento:
Curioso: a acurácia até subiu em OOT (0.54 vs 0.64 no modelo inicial), mas caiu bastante em DEV (0.47 vs 0.67 antes).
Isso mostra que o modelo ficou menos “ajustado” à amostra de treino e mais conservador/estável em produção.
"""

def decile_table(y, p, n=10):
    """Tabela de decil: do risco mais alto (1º decil) ao mais baixo (10º)."""
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    df_tmp['decile'] = pd.qcut(df_tmp['p'].rank(method='first', ascending=False),
                               q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']

    g = g.sort_index(ascending=True)  # 1 = piores clientes
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()

    g['gain'] = g['cum_pct_bad']
    overall_bad_rate = df_tmp['y'].mean()
    g['lift'] = g['bad_rate'] / overall_bad_rate
    return g.reset_index()

# Tabelas DEV e OOT
dec_dev_final = decile_table(y_dev, p_dev, n=10)
dec_oot_final = decile_table(y_oot, p_oot, n=10)

print("=== DECIL DEV ===")
display(dec_dev_final)

print("=== DECIL OOT ===")
display(dec_oot_final)

import matplotlib.pyplot as plt

# --- Gains Curve ---
def plot_gains(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.plot(dec_table['decile'], dec_table['gain'], marker='o', label=f'Gains {label}')
    plt.plot([1,10],[0,1],'--', color='gray', label='Aleatório')
    plt.title(f'Gains Curve — {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('% acumulado de maus capturados')
    plt.xticks(range(1,11))
    plt.ylim(0,1.05)
    plt.legend()
    plt.grid(True)
    plt.show()

# --- Lift Chart ---
def plot_lift(dec_table, label="DEV"):
    plt.figure(figsize=(6,5))
    plt.bar(dec_table['decile'], dec_table['lift'], color='skyblue')
    plt.axhline(1, color='red', linestyle='--', label='Aleatório')
    plt.title(f'Lift Chart — {label}')
    plt.xlabel('Decil (1 = piores clientes)')
    plt.ylabel('Lift')
    plt.xticks(range(1,11))
    plt.legend()
    plt.grid(True, axis='y')
    plt.show()

# ---- Executar para DEV e OOT ----
plot_gains(dec_dev_final, "DEV")
plot_gains(dec_oot_final, "OOT")

plot_lift(dec_dev_final, "DEV")
plot_lift(dec_oot_final, "OOT")

"""O modelo consegue concentrar ~70% dos maus na metade da carteira.

Lift >1,5 nos primeiros decis confirma poder discriminante útil.

DEV vs OOT → comportamento consistente, sem degradação relevante.

# 📑 Relatório Final — Modelo de Credit Scoring (Regressão Logística)

## 1. Introdução
O objetivo deste exercício foi desenvolver e avaliar um modelo de **credit scoring** utilizando **regressão logística**.  
Seguindo a lógica do professor, foram tratados **dados missing, outliers, zeros estruturais** e realizadas transformações supervisionadas (WOE/IV), com destaque para a variável **renda**, que apresentava instabilidade.

---

## 2. Amostragem
- **Base de Desenvolvimento (DEV):** primeiras safras.  
- **Base Out of Time (OOT):** 3 últimas safras (validação temporal).  
- Exclusões:  
  - `data_ref` (indicador de safra, não explicativa).  
  - `index` (identificador do cliente).  
- Target: `mau` (inadimplência).

---

## 3. Tratamento de variáveis
- **Zeros estruturais:** tratados como missing ou flags.  
- **Outliers:** winsorização em 1% e 99%.  
- **Categorias raras:** agrupadas em “OUTROS”.  
- **Renda:**  
  - Identificada como variável mais instável (PSI ≈ 2.1).  
  - Rebinada supervisionadamente em **6 faixas WOE**, resultando em **PSI ≈ 0.08** (estável).  
  - Modelo final utilizou **`renda_woe`**.

---

## 4. Avaliação do Modelo

### 4.1 Métricas Globais
| Base | AUC | Gini | KS | Acurácia | PSI Global |
|------|-----|------|----|----------|-------------|
| DEV  | 0.650 | 0.300 | 0.207 | 0.477 | 0.0000 |
| OOT  | 0.652 | 0.304 | 0.212 | 0.544 | 0.0000 |

- **AUC/Gini:** poder discriminante moderado.  
- **KS:** >0.20, indicando separação aceitável de bons/maus.  
- **PSI Global:** ≈ 0.0 → modelo **totalmente estável**.  

---

### 4.2 Tabelas de Decis

#### DEV
| Decil | Total | Bad | Bad Rate | Gain | Lift |
|-------|-------|-----|----------|------|------|
| 1     | 60.000 | 7.223 | 12.0% | 19.7% | 1.97 |
| 2     | 60.000 | 5.522 | 9.2%  | 34.8% | 1.51 |
| 3     | 60.000 | 4.743 | 7.9%  | 47.7% | 1.29 |
| 4     | 60.000 | 4.093 | 6.8%  | 58.9% | 1.12 |
| 5     | 60.000 | 3.767 | 6.3%  | 69.2% | 1.03 |
| ...   | ...   | ...   | ...    | ...  | ...  |
| 10    | 60.000 |   582 | 1.0%  | 100% | 0.16 |

#### OOT
| Decil | Total | Bad | Bad Rate | Gain | Lift |
|-------|-------|-----|----------|------|------|
| 1     | 15.000 | 4.043 | 27.0% | 18.4% | 1.84 |
| 2     | 15.000 | 3.233 | 21.6% | 33.1% | 1.47 |
| 3     | 15.000 | 2.800 | 18.7% | 45.8% | 1.27 |
| 4     | 15.000 | 2.537 | 16.9% | 57.3% | 1.15 |
| 5     | 15.000 | 2.345 | 15.6% | 68.0% | 1.07 |
| ...   | ...   | ...   | ...    | ...  | ...  |
| 10    | 15.000 |   396 | 2.6%  | 100% | 0.18 |

---

### 4.3 Gráficos Gains & Lift

- **Gains Curve:**  
  - Até o 3º decil, o modelo captura ~46–48% dos maus.  
  - Até o 5º decil, captura ~68–69%.  
  - Boa concentração inicial de risco.

- **Lift Chart:**  
  - Decil 1 apresenta Lift ~2, ou seja, clientes desse grupo têm **risco 2x maior que a média**.  
  - Lift decai gradualmente até 1 no último decil (clientes mais seguros).  

---

## 5. Conclusões
- O modelo final é **estável (PSI ~0.0)** e tem **poder discriminante moderado (AUC ~0.65, KS ~0.21)**.  
- A variável **renda**, rebinada supervisionadamente, foi essencial para garantir estabilidade sem perda relevante de poder preditivo.  
- O modelo concentra **~70% dos maus em 50% da carteira**, sendo útil para decisões de aprovação de crédito ou definição de limites.

---

## 6. Recomendações
1. **Monitorar continuamente o PSI** (global e por variável) a cada safra.  
2. **Ajustar binning da renda** se o perfil da base mudar (ex.: concentração maior em faixas baixas).  
3. Explorar **WOE supervisionado em outras variáveis** (ex.: idade, tempo de emprego) para possivelmente aumentar AUC/Gini.  
4. Testar modelos alternativos (**árvores de decisão, gradient boosting**) para verificar se há ganho de discriminância sem sacrificar estabilidade.  

---

# a - Criar um pipeline utilizando o sklearn pipeline para o preprocessamento

## Pré processamento

### Substituição de nulos (nans)

Existe nulos na base? é dado numérico ou categórico? qual o valor de substituição? média? valor mais frequente? etc

1. Existe nulos na base?

Sim, temos valores NaN em algumas variáveis:

Numéricas: por exemplo, tempo_emprego pode ter valores faltantes.

Categóricas: em colunas como educacao, estado_civil, etc.

2. É dado numérico ou categórico?

Numéricos: idade, tempo_emprego, renda, qt_pessoas_residencia, etc.

Categóricos: sexo, posse_de_veiculo, posse_de_imovel, tipo_renda, educacao, estado_civil, tipo_residencia.

3. Qual valor de substituição?

Numéricos → usar a mediana:

A mediana é robusta contra outliers e mantém a distribuição.

Categóricos → usar o valor mais frequente (moda):

Garante que não criamos categorias artificiais.
"""

# imputaçãode nulos
num_imputer = SimpleImputer(strategy='median')
cat_imputer = Pipeline(steps=[
    ('imp', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))
])

preproc = ColumnTransformer(transformers=[
    ('num', num_imputer, num_cols),
    ('cat', cat_imputer, cat_cols)
])

"""### Remoção de outliers

Como identificar outlier? Substituir o outlier por algum valor? Remover a linha?

Identificação: quantis (1%–99%) ou IQR.

Tratamento: winsorização (substituir pelo limite).

Remoção de linha: não recomendado.
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin

class OutlierCapping(BaseEstimator, TransformerMixin):
    def __init__(self, numeric_cols, q_low=0.01, q_high=0.99):
        self.numeric_cols = numeric_cols
        self.q_low = q_low
        self.q_high = q_high
        self.bounds_ = {}

    def fit(self, X, y=None):
        X = pd.DataFrame(X)
        for c in self.numeric_cols:
            ql, qh = X[c].quantile([self.q_low, self.q_high])
            self.bounds_[c] = (ql, qh)
        return self

    def transform(self, X):
        X = pd.DataFrame(X).copy()
        for c, (ql, qh) in self.bounds_.items():
            X[c] = X[c].clip(lower=ql, upper=qh)
        return X


from sklearn.pipeline import Pipeline

num_cols = [c for c in FEATURES if pd.api.types.is_numeric_dtype(df[c])]

pipe_outlier = Pipeline(steps=[
    ('outliers', OutlierCapping(numeric_cols=num_cols, q_low=0.01, q_high=0.99))
])

"""### Seleção de variáveis

Qual tipo de técnica? Boruta? Feature importance?

Vamos utilizar IV (Information Value) e a análise de significância das variáveis na regressão logística para seleção. Técnicas como Boruta ou Feature Importance podem ser exploradas como benchmark, mas em scoring a prioridade é interpretabilidade.
"""

import numpy as np
import pandas as pd

# ---------------- Utils: WOE/IV core ----------------
def _woe_iv_from_groups(groups):
    """Recebe DataFrame com colunas: count, bad, good. Devolve groups com WOE/IV e IV total."""
    g = groups.copy()
    total_good = g['good'].sum()
    total_bad  = g['bad'].sum()
    # proporções (evita zero)
    g['dist_good'] = (g['good'] / total_good).replace(0, 1e-6)
    g['dist_bad']  = (g['bad']  / total_bad ).replace(0, 1e-6)
    g['WOE'] = np.log(g['dist_good'] / g['dist_bad'])
    g['IV']  = (g['dist_good'] - g['dist_bad']) * g['WOE']
    return g, g['IV'].sum()

def _safe_edges_from_series(s, qbins=10):
    """Gera edges estritamente crescentes para binning por quantis, mesmo com empates/constantes."""
    s = pd.Series(s).dropna().astype(float)
    if s.empty:
        return np.array([0.0, 1.0])
    vmin, vmax = float(s.min()), float(s.max())
    if np.isclose(vmin, vmax):
        eps = max(1e-9, abs(vmin)*1e-6)
        return np.array([vmin - eps, vmax + eps])
    probs = np.linspace(0, 1, qbins+1)
    edges = np.quantile(s, probs)
    edges = np.unique(edges)
    if len(edges) < 2:
        edges = np.linspace(vmin, vmax, qbins+1)
    edges = np.unique(edges).astype(float)
    # força monotonicidade estrita
    for i in range(1, len(edges)):
        if edges[i] <= edges[i-1]:
            edges[i] = np.nextafter(edges[i-1], np.inf)
    return edges

# ------------- IV para uma coluna --------------------
def iv_for_numeric(df, col, target, qbins=10):
    s = pd.to_numeric(df[col], errors='coerce')
    y = df[target].astype(int)
    edges = _safe_edges_from_series(s, qbins=qbins)
    bins = pd.cut(s, bins=edges, include_lowest=True, right=True)
    # inclui bucket MISSING explícito
    bins = bins.astype(object)
    bins[s.isna()] = "MISSING"
    grp = pd.DataFrame({'bin': bins, 'y': y}).groupby('bin')['y'].agg(['count','sum'])
    grp.rename(columns={'sum':'bad'}, inplace=True)
    grp['good'] = grp['count'] - grp['bad']
    woe_table, iv_total = _woe_iv_from_groups(grp)
    return iv_total, woe_table

def iv_for_categorical(df, col, target, min_freq=0.01):
    s = df[col].astype('string')
    y = df[target].astype(int)
    s = s.fillna("MISSING")
    vc = s.value_counts(normalize=True, dropna=False)
    keep = set(vc[vc >= min_freq].index.tolist())
    s_grp = np.where(s.isin(keep), s, "OTHER")
    grp = pd.DataFrame({'bin': s_grp, 'y': y}).groupby('bin')['y'].agg(['count','sum'])
    grp.rename(columns={'sum':'bad'}, inplace=True)
    grp['good'] = grp['count'] - grp['bad']
    woe_table, iv_total = _woe_iv_from_groups(grp)
    return iv_total, woe_table

# ------------- IV para o DataFrame inteiro -----------
def compute_iv_all(df, target='target', exclude=None, qbins=10, min_freq_cat=0.01):
    exclude = set(exclude) if exclude else set()
    cols = [c for c in df.columns if c not in exclude and c != target]
    rows = []
    woe_tables = {}
    for col in cols:
        try:
            if pd.api.types.is_numeric_dtype(df[col]):
                iv, tab = iv_for_numeric(df, col, target, qbins=qbins)
                tipo = 'numérica'
            else:
                iv, tab = iv_for_categorical(df, col, target, min_freq=min_freq_cat)
                tipo = 'categórica'
            rows.append({'variavel': col, 'tipo': tipo, 'IV': iv})
            tab = tab.copy()
            tab.index = tab.index.astype(str)
            woe_tables[col] = tab[['count','bad','good','WOE','IV']].sort_values('WOE')
        except Exception as e:
            rows.append({'variavel': col, 'tipo': 'erro', 'IV': np.nan})
    iv_ranking = pd.DataFrame(rows).sort_values('IV', ascending=False).reset_index(drop=True)
    return iv_ranking, woe_tables

# ===================== EXECUTAR ======================
# Ajuste o EXCLUDE conforme seu fluxo
EXCLUDE = {'data_ref','_ref_date','index','mau','renda_woe'}
if 'target' not in df.columns and 'mau' in df.columns:
    df['target'] = df['mau'].astype(int)

iv_ranking, woe_tables = compute_iv_all(
    df=df,
    target='target',
    exclude=EXCLUDE,
    qbins=10,
    min_freq_cat=0.01
)

display(iv_ranking.head(20))

var_top = iv_ranking.iloc[0]['variavel']
print("Top variável por IV:", var_top)
display(woe_tables[var_top])

"""### Redução de dimensionalidade (PCA)

Aplicar PCA para reduzir a dimensionalidade para 5

## Seleção de Variáveis – Redução de Dimensionalidade (PCA)

### 🎓 Exercício
No exercício foi solicitado aplicar **PCA** para reduzir a dimensionalidade para **5 componentes**.  
- A técnica escolhida foi **Análise de Componentes Principais (PCA)**.  
- O número de componentes foi definido pelo enunciado (**5**).  
- Alternativamente, poderíamos escolher o número de componentes com base na **variância explicada acumulada** (ex.: manter 80–95%).  
- Após a transformação, o modelo foi treinado sobre os 5 componentes principais.

➡️ **Resposta ao exercício:** aplicamos PCA(5) como técnica de seleção/redução.

---

### 💼 Mercado
Na prática de **credit scoring regulatório**, o uso de PCA é raro porque:
- Os componentes não são interpretáveis, dificultando a **explicação do risco ao regulador**.  
- Normas de auditoria (Bacen, IFRS 9, Basel II/III) exigem que cada variável usada seja **explicável e justificável**.  

No mercado, a seleção de variáveis é feita por métodos que preservam interpretabilidade:
- **Information Value (IV)** e **Weight of Evidence (WOE)** para medir o poder preditivo de cada variável.  
- **Teste de significância na regressão logística** (p-valores).  
- **Análise de correlação** para evitar colinearidade.  
- **Penalização L1 (Lasso)** pode ser usada como apoio.

➡️ **Resumo mercado:** a seleção é feita por **IV/WOE + correlação + significância estatística**, mantendo interpretabilidade e transparência.

---

### ✅ Conclusão
- **No exercício**: PCA(5) foi usado para demonstrar a técnica de redução de dimensionalidade.  
- **No mercado**: PCA não é usual em scoring; preferem-se técnicas supervisionadas (IV/WOE) e análise estatística para selecionar variáveis.

### Criação de dummies

Aplicar o get_dummies() ou onehotencoder() para transformar colunas catégoricas do dataframe em colunas de 0 e 1.
- sexo
- posse_de_veiculo
- posse_de_imovel
- tipo_renda
- educacao
- estado_civil
- tipo_residencia

Utilizamos One-Hot Encoding para converter variáveis categóricas em dummies (0/1).

Evitamos multicolinearidade aplicando drop_first=True.

Esse processo garante que variáveis qualitativas possam ser usadas no modelo de regressão logística.
"""

cat_vars = [
    "sexo", "posse_de_veiculo", "posse_de_imovel",
    "tipo_renda", "educacao", "estado_civil", "tipo_residencia"
]

df_dummies = pd.get_dummies(df, columns=cat_vars, drop_first=True)
df_dummies.head()

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

cat_vars = [
    "sexo", "posse_de_veiculo", "posse_de_imovel",
    "tipo_renda", "educacao", "estado_civil", "tipo_residencia"
]

num_vars = [c for c in df.columns if c not in cat_vars and c not in {"data_ref","_ref_date","index","mau","target"}]

preproc = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='median'), num_vars),
    ('cat', Pipeline(steps=[
        ('imp', SimpleImputer(strategy='most_frequent')),
        ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
    ]), cat_vars)
])

pipe = Pipeline(steps=[
    ('preproc', preproc),
    ('clf', LogisticRegression(max_iter=1000, solver='lbfgs'))
])

X_dev, y_dev = df_dev[num_vars+cat_vars], df_dev['target']
pipe.fit(X_dev, y_dev)

"""### Pipeline

Crie um pipeline contendo essas funções.

preprocessamento()
- substituicao de nulos
- remoção outliers
- PCA
- Criação de dummy de pelo menos 1 variável (posse_de_veiculo)
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA

# ---------- Pipes menores ----------
# Numéricas: imputação + scaler (+ PCA opcional)
num_pipe = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
])

# Categóricas: imputação + OneHot
cat_pipe = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(drop="first", handle_unknown="ignore", sparse_output=False))
])

# Combina numéricas e categóricas

# Redefine num_cols and cat_cols based on the columns in X_dev_prep
num_cols = [c for c in X_dev_prep.columns if pd.api.types.is_numeric_dtype(X_dev_prep[c])]
cat_cols = [c for c in X_dev_prep.columns if c not in num_cols]


preproc = ColumnTransformer(
    transformers=[
        ("num", num_pipe, num_cols),
        ("cat", cat_pipe, cat_cols)
    ]
)

# Modelo final
logistic_pipe = LogisticRegression(max_iter=1000, solver="lbfgs")

# ---------- Pipeline completo ----------
pipe = Pipeline(steps=[
    ("preprocessamento", preproc),
    ("logistic", logistic_pipe)
])

# Treino
pipe.fit(X_dev_prep, y_dev)

# Avaliação rápida
p_dev = pipe.predict_proba(X_dev_prep)[:, 1]
p_oot = pipe.predict_proba(X_oot_prep)[:, 1]

print("AUC DEV:", roc_auc_score(y_dev, p_dev))
print("AUC OOT:", roc_auc_score(y_oot, p_oot))

import joblib

# salva em disco
joblib.dump(pipe, "model_final.pkl")
print("Modelo salvo em model_final.pkl")

"""# b - Pycaret na base de dados

Utilize o pycaret para pre processar os dados e rodar o modelo **lightgbm**. Faça todos os passos a passos da aula e gere os gráficos finais. E o pipeline de toda a transformação.

## 1) Instalação
"""

!pip install git+https://github.com/pycaret/pycaret.git@master --upgrade

"""## 2) Preparação: features, DEV e OO"""

import numpy as np
import pandas as pd

# Garante target (0/1)
for d in (df_dev, df_oot):
    if 'target' not in d.columns and 'mau' in d.columns:
        d['target'] = d['mau'].astype(int)

# Colunas a excluir do modelo
EXCLUDE = {'data_ref','_ref_date','index','mau','target'}

# (opcional) se você já usa renda_woe e quer tirar renda crua:
if 'renda_woe' in df_dev.columns:
    EXCLUDE = EXCLUDE.union({'renda'})

# Colunas em comum entre DEV e OOT
cols_dev = set(df_dev.columns) - EXCLUDE
cols_oot = set(df_oot.columns) - EXCLUDE
FEATURES = sorted(cols_dev & cols_oot)

print("Features:", len(FEATURES))

"""## 3) Setup do PyCaret (base DEV)"""

from pycaret.classification import setup, create_model, tune_model, finalize_model, plot_model, pull, save_model, load_model, predict_model, get_config

data_dev = df_dev[FEATURES + ['target']].copy()
s = setup(
    data=data_dev,
    target='target',
    session_id=42,
    imputation_type='simple',
    numeric_imputation='median',
    categorical_imputation='mode',
    preprocess=True,
    normalize=False,
    pca=False,
    fold=3,
    fold_shuffle=True,
    verbose=False
)
print("Setup OK")

"""## 4) Modelo LightGBM (criar → tunar → finalizar)"""

!pip install "pycaret[tuners]"

print("n FEATURES:", len(FEATURES))
print("Exemplo FEATURES:", FEATURES[:10])
print("DEV target balance:", df_dev['target'].value_counts(normalize=True).to_dict())
print("OOT target balance:", df_oot['target'].value_counts(normalize=True).to_dict())

# 4.1) Criar modelo base LightGBM
lgbm = create_model(
    'lightgbm',
    cross_validation=False,
    n_estimators=300,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight='balanced',
    verbose=False
)

# 4.2) Tunagem rápida (ajuste de hiperparâmetros)
from pycaret.classification import tune_model
lgbm_tuned = tune_model(
    lgbm,
    search_library='scikit-optimize',
    n_iter=10,
    fold=3,
    optimize='AUC',
    choose_better=True,
    verbose=False
)

# 4.3) Finalizar modelo (fit em todo DEV)
lgbm_final = finalize_model(lgbm_tuned)

"""## 6) Avaliação DEV e OOT (AUC, Gini, KS, ACC)"""

import numpy as np
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix

# --- 1) pegar a coluna certa do PyCaret ---
def extract_proba(df_pred):
    for c in ['prediction_score', 'Score', 'score']:
        if c in df_pred.columns:
            return df_pred[c].values.astype(float)
    raise KeyError(f"Nenhuma coluna de score encontrada em {df_pred.columns.tolist()}")

pred_dev = predict_model(lgbm_final, data=df_dev[FEATURES].copy())
pred_oot = predict_model(lgbm_final, data=df_oot[FEATURES].copy())

p_dev = extract_proba(pred_dev)
p_oot = extract_proba(pred_oot)

y_dev = df_dev['target'].astype(int).values
y_oot = df_oot['target'].astype(int).values

# --- 2) utilitários robustos ---
def youden_thr_safe(y, p):
    if np.allclose(p, p[0]):
        return 0.5
    fpr, tpr, thr = roc_curve(y, p)
    return thr[np.argmax(tpr - fpr)]

def ensure_positive_proba(y, p):
    """Garante que p é P(y=1). Se AUC<0.5, inverte e retorna p_corrigido."""
    auc = roc_auc_score(y, p)
    flipped = False
    if auc < 0.5:
        p = 1 - p
        flipped = True
    return p, flipped

def avaliar(y, p, lbl):
    # corrige inversão se necessário
    p, flipped = ensure_positive_proba(y, p)
    # threshold de Youden com fallback
    thr = youden_thr_safe(y, p)
    yhat = (p >= thr).astype(int)
    auc = roc_auc_score(y, p); gini = 2*auc - 1
    fpr, tpr, _ = roc_curve(y, p); ks = (tpr - fpr).max()
    acc = accuracy_score(y, yhat); cm = confusion_matrix(y, yhat)
    flip_msg = " (prob invertida corrigida)" if flipped else ""
    print(f"[{lbl}{flip_msg}] thr={thr:.4f} | ACC={acc:.4f} | AUC={auc:.4f} | Gini={gini:.4f} | KS={ks:.4f}")
    print("Confusion:\n", cm)
    return dict(thr=thr, acc=acc, auc=auc, gini=gini, ks=ks, cm=cm, flipped=flipped)

# --- 3) avaliação DEV e OOT ---
res_dev = avaliar(y_dev, p_dev, "DEV (PyCaret LGBM)")
res_oot = avaliar(y_oot, p_oot, "OOT (PyCaret LGBM)")

"""## 7) Tabela de Decis (Gains/Lift) para DEV e OOT"""

def decile_table(y, p, n=10):
    df_tmp = pd.DataFrame({'y': y, 'p': p})
    df_tmp['rank'] = df_tmp['p'].rank(method='first', ascending=False)
    df_tmp['decile'] = pd.qcut(df_tmp['rank'], q=n, labels=False) + 1

    g = df_tmp.groupby('decile').agg(total=('y','count'),
                                     bad=('y','sum'))
    g['good'] = g['total'] - g['bad']
    g['bad_rate'] = g['bad'] / g['total']
    g = g.sort_index()
    g['cum_total'] = g['total'].cumsum()
    g['cum_bad'] = g['bad'].cumsum()
    g['cum_pct_total'] = g['cum_total'] / g['total'].sum()
    g['cum_pct_bad'] = g['cum_bad'] / g['bad'].sum()
    overall_bad_rate = df_tmp['y'].mean()
    g['gain'] = g['cum_pct_bad']
    g['lift'] = g['bad_rate'] / overall_bad_rate
    return g.reset_index()

dec_dev = decile_table(y_dev, p_dev, n=10)
dec_oot = decile_table(y_oot, p_oot, n=10)

print("=== DECIL DEV ==="); display(dec_dev)
print("=== DECIL OOT ==="); display(dec_oot)

"""## 8) PSI global (probabilidades DEV vs OOT)"""

def psi_global(expected, actual, bins=10):
    e, _ = np.histogram(expected, bins=bins, range=(0,1), density=True)
    a, _ = np.histogram(actual,   bins=bins, range=(0,1), density=True)
    e = e / (e.sum() + 1e-12); a = a / (a.sum() + 1e-12)
    e = np.where(e==0, 1e-6, e); a = np.where(a==0, 1e-6, a)
    return float(np.sum((a - e) * np.log(a / e)))

print("PSI DEV vs OOT:", round(psi_global(p_dev, p_oot, bins=10), 4))

"""## 9) Exportar pipeline completo (transformações + LightGBM)"""

# Salva o pipeline completo (toda a cadeia de pré-processamento + modelo LGBM)
save_model(lgbm_final, "pycaret_lgbm_pipeline_220925")
print("Arquivos salvos: pycaret_lgbm_pipeline.pkl/.pkl")

"""# Projeto Final

1. Subir no GITHUB todos os jupyter notebooks/códigos que você desenvolveu nesse ultimo módulo
1. Gerar um arquivo python (.py) com todas as funções necessárias para rodar no streamlit a escoragem do arquivo de treino
    - Criar um .py
    - Criar um carregador de csv no streamlit
    - Subir um csv no streamlit
    - Criar um pipeline de pré processamento dos dados
    - Utilizar o modelo treinado para escorar a base
        - nome_arquivo = 'model_final.pkl'
1. Gravar um vídeo da tela do streamlit em funcionamento (usando o próprio streamlit (temos aula disso) ou qlqr outra forma de gravação).
1. Subir no Github o vídeo de funcionamento da ferramenta como README.md.
1. Subir no Github os códigos desenvolvidos.
1. Enviar links do github para o tutor corrigir.
"""